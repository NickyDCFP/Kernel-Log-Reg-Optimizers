{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: str = './dataset/'\n",
    "dataset_filename: str = 'data1.mat'\n",
    "data = scipy.io.loadmat(os.path.join(dataset_path, dataset_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X: np.ndarray = data['TrainingX']\n",
    "train_Y: np.ndarray = data['TrainingY'].flatten()\n",
    "test_X: np.ndarray = data['TestX']\n",
    "test_Y: np.ndarray = data['TestY'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v: np.ndarray) -> np.ndarray:\n",
    "    \"The sigmoid function, applied to input v.\"\n",
    "    return 1 / (1 + np.exp(-v))\n",
    "\n",
    "def loss(w: np.ndarray, k: np.ndarray, y: np.ndarray, lamb: float) -> float:\n",
    "    \"\"\"\n",
    "    The logistic loss function with L2 regularization. Computes for weight\n",
    "    vector w on kernel input k with labels y and regularization parameter lamb.\n",
    "    \"\"\"\n",
    "    return -np.sum(np.log(sigmoid(y * k.T.dot(w)))) + lamb * w.T.dot(w)\n",
    "\n",
    "def gradient(w: np.ndarray, k: np.ndarray, y: np.ndarray, lamb: float) -> float:\n",
    "    \"\"\"\n",
    "    The gradient of the logistic loss function with L2 regularization. Computes\n",
    "    for weight vector w on kernel input k with labels y and regularization\n",
    "    parameter lamb.\n",
    "    \"\"\"\n",
    "    return -np.sum(y * (1 - sigmoid(y * k.T.dot(w) + 1e-10)) * k, axis=1) + 2 * lamb * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma_squared(X: np.ndarray) -> float:\n",
    "    \"Computes the sigma squared value for an input matrix X.\"\n",
    "    pdst = squareform(pdist(X, metric='sqeuclidean'))\n",
    "    return np.sum(pdst) / (X.shape[0] ** 2)\n",
    "\n",
    "class Kernel:\n",
    "    \"\"\"\n",
    "    Kernel function fit to an input X matrix and can be applied as a\n",
    "    transform to a corresponding Y matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray):\n",
    "        var = get_sigma_squared(X)\n",
    "        self.gamma = 1 / (2 * var)\n",
    "        self.x = X\n",
    "    \n",
    "    def __call__(self, Y: np.ndarray) -> np.ndarray:\n",
    "        return rbf_kernel(self.x, Y, self.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_kernels(\n",
    "    train_X: np.ndarray,\n",
    "    test_X: np.ndarray,\n",
    "    subsample: bool = False,\n",
    "    train_Y: np.ndarray = None,\n",
    "    samples_per_class: int = 2000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieves the train and test kernels, subsampling the original dataset if\n",
    "    necessary.\n",
    "\n",
    "    Parameters:\n",
    "        train_X:            The training inputs\n",
    "        test_X:             The test inputs\n",
    "        subsample:          The number of samples to subsample per class\n",
    "        train_Y:            The training targets for subsampling\n",
    "        samples_per_class:  The number of samples in each class\n",
    "    \n",
    "    Returns:\n",
    "        The kernels for the train and test sets\n",
    "    \"\"\"\n",
    "    if subsample:\n",
    "        assert train_Y is not None, \\\n",
    "            \"Must provide train Y values for subsampling\"\n",
    "        one_indices: np.ndarray = np.arange(train_Y.shape[0])[train_Y == 1]\n",
    "        neg_indices: np.ndarray = np.arange(train_Y.shape[0])[train_Y == -1]\n",
    "        one_indices = np.random.permutation(one_indices)[:samples_per_class]\n",
    "        neg_indices = np.random.permutation(neg_indices)[:samples_per_class]\n",
    "        subset: np.ndarray = train_X[np.concatenate((one_indices, neg_indices))]\n",
    "        kernel: Kernel = Kernel(subset)\n",
    "        return kernel(train_X), kernel(test_X)\n",
    "\n",
    "    kernel: Kernel = Kernel(train_X)\n",
    "    return kernel(train_X), kernel(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    k: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lamb: float,\n",
    "    lr: float,\n",
    "    opt: str,\n",
    "    p: float = 1,\n",
    "    maxcor: int = 10\n",
    ") -> tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Trains a weight vector on the dataset k and y with regularization\n",
    "    parameter lambda, learning rate lr, and optimizer opt.\n",
    "\n",
    "    Parameters:\n",
    "        k:      The input kernel for the logistic regressor\n",
    "        y:      The targets for the logistic regressor\n",
    "        lamb:   The lambda parameter\n",
    "        lr:     The learning rate parameters\n",
    "        opt:    The optimizer in string form\n",
    "        p:      Batch size for SGD\n",
    "        maxcor: The number of vectors to use for LBFGS\n",
    "    \n",
    "    Returns:\n",
    "        The history of the training\n",
    "    \"\"\"\n",
    "    epoch: int = 1\n",
    "    history: pd.DataFrame = pd.DataFrame(columns=['Train Loss'])\n",
    "    def iter_callback(w: np.ndarray):\n",
    "        print(f\"Epoch {iter_callback.iter}\")\n",
    "        train_loss: float = loss(w, k, y, lamb)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        iter_callback.history.loc[len(iter_callback.history)] = train_loss\n",
    "        iter_callback.iter += 1\n",
    "    iter_callback.iter = 1\n",
    "    iter_callback.history = pd.DataFrame(columns=['Train Loss'])\n",
    "    w: np.ndarray = np.zeros(k.shape[0])\n",
    "\n",
    "    if opt == \"gd\":\n",
    "        while True:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            train_loss: float = loss(w, k, y, lamb)\n",
    "            print(f\"Train Loss: {train_loss}\")\n",
    "            history.loc[len(history)] = train_loss\n",
    "            grad = gradient(w, k, y, lamb)\n",
    "            w -= lr * grad\n",
    "            if np.linalg.norm(grad) <= 1e-5:\n",
    "                break\n",
    "            epoch += 1\n",
    "    elif opt == \"sgd\":\n",
    "        old_loss: float = np.inf\n",
    "        lr_decay: bool = False # decay only once loss changes start to slow\n",
    "        while True:\n",
    "            # Record train loss\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            train_loss: float = loss(w, k, y, lamb)\n",
    "            print(f\"Train Loss: {train_loss}\")\n",
    "            history.loc[len(history)] = train_loss\n",
    "            \n",
    "            # Decide if we should start decaying LR\n",
    "            loss_delta: float = np.abs(train_loss - old_loss)\n",
    "            if loss_delta <= 0.001 or old_loss < train_loss:\n",
    "                lr_decay = True\n",
    "            old_loss = train_loss\n",
    "            dyn_lr = lr / (epoch ** 3)\n",
    "\n",
    "            rng: np.ndarray = np.random.permutation(np.arange(k.shape[0]))\n",
    "            for batch in range(k.shape[0] // p):\n",
    "                # Create batch\n",
    "                batch_range: list[int] = [batch * p, (batch + 1) * p]\n",
    "                batch_indices = rng[batch_range[0]:batch_range[1]]\n",
    "                batch_K = k[:, batch_indices]\n",
    "                batch_Y = y[batch_indices]\n",
    "\n",
    "                # Split up weight decay across batches\n",
    "                lamb_grad = lamb / (k.shape[0] / p)\n",
    "\n",
    "                # Gradient update\n",
    "                grad =  gradient(w, batch_K, batch_Y, lamb_grad)\n",
    "                step = dyn_lr * grad if lr_decay else lr * grad\n",
    "                w -= step\n",
    "\n",
    "                # Decide if we should stop\n",
    "                if np.linalg.norm(step) <= 1e-5:\n",
    "                    end = True\n",
    "                    break\n",
    "                else:\n",
    "                    end = False\n",
    "            if end:\n",
    "                break\n",
    "            epoch += 1\n",
    "    elif opt == \"bfgs\":\n",
    "        w = minimize(\n",
    "            fun=loss,\n",
    "            x0=w,\n",
    "            args=(k, y, lamb),\n",
    "            method='BFGS',\n",
    "            jac=gradient,\n",
    "            callback=iter_callback,\n",
    "        ).x\n",
    "        history = iter_callback.history\n",
    "    elif opt == \"lbfgs\":\n",
    "        w = minimize(\n",
    "            fun=loss,\n",
    "            x0=w,\n",
    "            args=(k, y, lamb),\n",
    "            method='L-BFGS-B',\n",
    "            jac=gradient,\n",
    "            callback=iter_callback,\n",
    "            options={\n",
    "                'maxcor' : maxcor\n",
    "            }\n",
    "        ).x\n",
    "        history = iter_callback.history\n",
    "    return w, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_K_unscaled: np.ndarray; test_K_unscaled: np.ndarray\n",
    "train_K: np.ndarray; test_K: np.ndarray\n",
    "train_K_unscaled, test_K_unscaled = get_train_test_kernels(train_X, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling helps values not blow up\n",
    "train_K = train_K_unscaled / 6000\n",
    "test_K = test_K_unscaled / 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfgs_K_unscaled: np.ndarray; bfgs_test_K_unscaled: np.ndarray\n",
    "bfgs_K: np.ndarray; bfgs_test_K: np.ndarray\n",
    "bfgs_K_unscaled, bfgs_test_K_unscaled = get_train_test_kernels(\n",
    "    train_X,\n",
    "    test_X,\n",
    "    subsample=True,\n",
    "    train_Y=train_Y\n",
    ")\n",
    "bfgs_K = bfgs_K_unscaled / 6000\n",
    "bfgs_test_K = bfgs_test_K_unscaled / 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb: float = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 6931.471805599453\n",
      "Epoch 2\n",
      "Train Loss: 6929.249496199183\n",
      "Epoch 3\n",
      "Train Loss: 6927.169055370142\n",
      "Epoch 4\n",
      "Train Loss: 6925.202348329961\n",
      "Epoch 5\n",
      "Train Loss: 6923.328433919629\n",
      "Epoch 6\n",
      "Train Loss: 6921.531681545782\n",
      "Epoch 7\n",
      "Train Loss: 6919.800381826521\n",
      "Epoch 8\n",
      "Train Loss: 6918.1257215699725\n",
      "Epoch 9\n",
      "Train Loss: 6916.501027567725\n",
      "Epoch 10\n",
      "Train Loss: 6914.921208704513\n",
      "Epoch 11\n",
      "Train Loss: 6913.382344363787\n",
      "Epoch 12\n",
      "Train Loss: 6911.881380749755\n",
      "Epoch 13\n",
      "Train Loss: 6910.4159068135405\n",
      "Epoch 14\n",
      "Train Loss: 6908.983988898615\n",
      "Epoch 15\n",
      "Train Loss: 6907.584048700407\n",
      "Epoch 16\n",
      "Train Loss: 6906.214773176934\n",
      "Epoch 17\n",
      "Train Loss: 6904.87504802898\n",
      "Epoch 18\n",
      "Train Loss: 6903.563908567408\n",
      "Epoch 19\n",
      "Train Loss: 6902.28050340742\n",
      "Epoch 20\n",
      "Train Loss: 6901.02406762593\n",
      "Epoch 21\n",
      "Train Loss: 6899.793902900859\n",
      "Epoch 22\n",
      "Train Loss: 6898.589362801966\n",
      "Epoch 23\n",
      "Train Loss: 6897.409841883086\n",
      "Epoch 24\n",
      "Train Loss: 6896.254767579809\n",
      "Epoch 25\n",
      "Train Loss: 6895.123594177795\n",
      "Epoch 26\n",
      "Train Loss: 6894.015798309786\n",
      "Epoch 27\n",
      "Train Loss: 6892.930875581391\n",
      "Epoch 28\n",
      "Train Loss: 6891.868338030657\n",
      "Epoch 29\n",
      "Train Loss: 6890.827712203813\n",
      "Epoch 30\n",
      "Train Loss: 6889.808537686572\n",
      "Epoch 31\n",
      "Train Loss: 6888.8103659725475\n",
      "Epoch 32\n",
      "Train Loss: 6887.832759581316\n",
      "Epoch 33\n",
      "Train Loss: 6886.875291361669\n",
      "Epoch 34\n",
      "Train Loss: 6885.937543932382\n",
      "Epoch 35\n",
      "Train Loss: 6885.019109225414\n",
      "Epoch 36\n",
      "Train Loss: 6884.119588105529\n",
      "Epoch 37\n",
      "Train Loss: 6883.238590047246\n",
      "Epoch 38\n",
      "Train Loss: 6882.3757328549045\n",
      "Epoch 39\n",
      "Train Loss: 6881.53064241543\n",
      "Epoch 40\n",
      "Train Loss: 6880.702952476049\n",
      "Epoch 41\n",
      "Train Loss: 6879.892304441219\n",
      "Epoch 42\n",
      "Train Loss: 6879.098347184567\n",
      "Epoch 43\n",
      "Train Loss: 6878.320736872656\n",
      "Epoch 44\n",
      "Train Loss: 6877.55913679828\n",
      "Epoch 45\n",
      "Train Loss: 6876.81321722151\n",
      "Epoch 46\n",
      "Train Loss: 6876.0826552172375\n",
      "Epoch 47\n",
      "Train Loss: 6875.367134528201\n",
      "Epoch 48\n",
      "Train Loss: 6874.66634542279\n",
      "Epoch 49\n",
      "Train Loss: 6873.97998455706\n",
      "Epoch 50\n",
      "Train Loss: 6873.307754840532\n",
      "Epoch 51\n",
      "Train Loss: 6872.649365305471\n",
      "Epoch 52\n",
      "Train Loss: 6872.0045309793495\n",
      "Epoch 53\n",
      "Train Loss: 6871.372972760334\n",
      "Epoch 54\n",
      "Train Loss: 6870.754417295611\n",
      "Epoch 55\n",
      "Train Loss: 6870.14859686243\n",
      "Epoch 56\n",
      "Train Loss: 6869.555249251735\n",
      "Epoch 57\n",
      "Train Loss: 6868.97411765432\n",
      "Epoch 58\n",
      "Train Loss: 6868.404950549415\n",
      "Epoch 59\n",
      "Train Loss: 6867.8475015956\n",
      "Epoch 60\n",
      "Train Loss: 6867.301529524048\n",
      "Epoch 61\n",
      "Train Loss: 6866.766798033971\n",
      "Epoch 62\n",
      "Train Loss: 6866.243075690255\n",
      "Epoch 63\n",
      "Train Loss: 6865.7301358232235\n",
      "Epoch 64\n",
      "Train Loss: 6865.2277564304695\n",
      "Epoch 65\n",
      "Train Loss: 6864.735720080727\n",
      "Epoch 66\n",
      "Train Loss: 6864.253813819726\n",
      "Epoch 67\n",
      "Train Loss: 6863.781829077987\n",
      "Epoch 68\n",
      "Train Loss: 6863.319561580534\n",
      "Epoch 69\n",
      "Train Loss: 6862.866811258445\n",
      "Epoch 70\n",
      "Train Loss: 6862.423382162246\n",
      "Epoch 71\n",
      "Train Loss: 6861.989082377084\n",
      "Epoch 72\n",
      "Train Loss: 6861.563723939643\n",
      "Epoch 73\n",
      "Train Loss: 6861.147122756778\n",
      "Epoch 74\n",
      "Train Loss: 6860.739098525825\n",
      "Epoch 75\n",
      "Train Loss: 6860.339474656545\n",
      "Epoch 76\n",
      "Train Loss: 6859.948078194687\n",
      "Epoch 77\n",
      "Train Loss: 6859.564739747112\n",
      "Epoch 78\n",
      "Train Loss: 6859.189293408473\n",
      "Epoch 79\n",
      "Train Loss: 6858.821576689394\n",
      "Epoch 80\n",
      "Train Loss: 6858.461430446133\n",
      "Epoch 81\n",
      "Train Loss: 6858.10869881169\n",
      "Epoch 82\n",
      "Train Loss: 6857.763229128344\n",
      "Epoch 83\n",
      "Train Loss: 6857.424871881562\n",
      "Epoch 84\n",
      "Train Loss: 6857.093480635281\n",
      "Epoch 85\n",
      "Train Loss: 6856.768911968523\n",
      "Epoch 86\n",
      "Train Loss: 6856.4510254133065\n",
      "Epoch 87\n",
      "Train Loss: 6856.139683393852\n",
      "Epoch 88\n",
      "Train Loss: 6855.834751167019\n",
      "Epoch 89\n",
      "Train Loss: 6855.536096763983\n",
      "Epoch 90\n",
      "Train Loss: 6855.2435909331125\n",
      "Epoch 91\n",
      "Train Loss: 6854.957107084024\n",
      "Epoch 92\n",
      "Train Loss: 6854.67652123277\n",
      "Epoch 93\n",
      "Train Loss: 6854.401711948192\n",
      "Epoch 94\n",
      "Train Loss: 6854.13256029934\n",
      "Epoch 95\n",
      "Train Loss: 6853.868949804\n",
      "Epoch 96\n",
      "Train Loss: 6853.6107663782705\n",
      "Epoch 97\n",
      "Train Loss: 6853.357898287184\n",
      "Epoch 98\n",
      "Train Loss: 6853.110236096332\n",
      "Epoch 99\n",
      "Train Loss: 6852.867672624504\n",
      "Epoch 100\n",
      "Train Loss: 6852.630102897293\n",
      "Epoch 101\n",
      "Train Loss: 6852.397424101649\n",
      "Epoch 102\n",
      "Train Loss: 6852.169535541384\n",
      "Epoch 103\n",
      "Train Loss: 6851.94633859358\n",
      "Epoch 104\n",
      "Train Loss: 6851.727736665897\n",
      "Epoch 105\n",
      "Train Loss: 6851.5136351547735\n",
      "Epoch 106\n",
      "Train Loss: 6851.303941404464\n",
      "Epoch 107\n",
      "Train Loss: 6851.098564666942\n",
      "Epoch 108\n",
      "Train Loss: 6850.897416062614\n",
      "Epoch 109\n",
      "Train Loss: 6850.7004085418475\n",
      "Epoch 110\n",
      "Train Loss: 6850.507456847301\n",
      "Epoch 111\n",
      "Train Loss: 6850.318477477005\n",
      "Epoch 112\n",
      "Train Loss: 6850.133388648234\n",
      "Epoch 113\n",
      "Train Loss: 6849.952110262093\n",
      "Epoch 114\n",
      "Train Loss: 6849.7745638688575\n",
      "Epoch 115\n",
      "Train Loss: 6849.600672634013\n",
      "Epoch 116\n",
      "Train Loss: 6849.430361304994\n",
      "Epoch 117\n",
      "Train Loss: 6849.263556178621\n",
      "Epoch 118\n",
      "Train Loss: 6849.1001850691855\n",
      "Epoch 119\n",
      "Train Loss: 6848.940177277217\n",
      "Epoch 120\n",
      "Train Loss: 6848.783463558876\n",
      "Epoch 121\n",
      "Train Loss: 6848.629976095977\n",
      "Epoch 122\n",
      "Train Loss: 6848.479648466649\n",
      "Epoch 123\n",
      "Train Loss: 6848.332415616571\n",
      "Epoch 124\n",
      "Train Loss: 6848.188213830818\n",
      "Epoch 125\n",
      "Train Loss: 6848.046980706288\n",
      "Epoch 126\n",
      "Train Loss: 6847.908655124687\n",
      "Epoch 127\n",
      "Train Loss: 6847.773177226077\n",
      "Epoch 128\n",
      "Train Loss: 6847.640488382965\n",
      "Epoch 129\n",
      "Train Loss: 6847.510531174924\n",
      "Epoch 130\n",
      "Train Loss: 6847.38324936375\n",
      "Epoch 131\n",
      "Train Loss: 6847.258587869108\n",
      "Epoch 132\n",
      "Train Loss: 6847.136492744703\n",
      "Epoch 133\n",
      "Train Loss: 6847.0169111549185\n",
      "Epoch 134\n",
      "Train Loss: 6846.899791351958\n",
      "Epoch 135\n",
      "Train Loss: 6846.785082653442\n",
      "Epoch 136\n",
      "Train Loss: 6846.672735420471\n",
      "Epoch 137\n",
      "Train Loss: 6846.562701036147\n",
      "Epoch 138\n",
      "Train Loss: 6846.454931884521\n",
      "Epoch 139\n",
      "Train Loss: 6846.349381329992\n",
      "Epoch 140\n",
      "Train Loss: 6846.246003697115\n",
      "Epoch 141\n",
      "Train Loss: 6846.14475425084\n",
      "Epoch 142\n",
      "Train Loss: 6846.045589177141\n",
      "Epoch 143\n",
      "Train Loss: 6845.948465564061\n",
      "Epoch 144\n",
      "Train Loss: 6845.85334138313\n",
      "Epoch 145\n",
      "Train Loss: 6845.760175471182\n",
      "Epoch 146\n",
      "Train Loss: 6845.668927512535\n",
      "Epoch 147\n",
      "Train Loss: 6845.579558021543\n",
      "Epoch 148\n",
      "Train Loss: 6845.492028325505\n",
      "Epoch 149\n",
      "Train Loss: 6845.406300547925\n",
      "Epoch 150\n",
      "Train Loss: 6845.322337592127\n",
      "Epoch 151\n",
      "Train Loss: 6845.240103125188\n",
      "Epoch 152\n",
      "Train Loss: 6845.159561562223\n",
      "Epoch 153\n",
      "Train Loss: 6845.080678050969\n",
      "Epoch 154\n",
      "Train Loss: 6845.003418456721\n",
      "Epoch 155\n",
      "Train Loss: 6844.927749347539\n",
      "Epoch 156\n",
      "Train Loss: 6844.853637979794\n",
      "Epoch 157\n",
      "Train Loss: 6844.781052283982\n",
      "Epoch 158\n",
      "Train Loss: 6844.709960850855\n",
      "Epoch 159\n",
      "Train Loss: 6844.640332917823\n",
      "Epoch 160\n",
      "Train Loss: 6844.572138355639\n",
      "Epoch 161\n",
      "Train Loss: 6844.50534765536\n",
      "Epoch 162\n",
      "Train Loss: 6844.439931915574\n",
      "Epoch 163\n",
      "Train Loss: 6844.375862829896\n",
      "Epoch 164\n",
      "Train Loss: 6844.313112674712\n",
      "Epoch 165\n",
      "Train Loss: 6844.251654297183\n",
      "Epoch 166\n",
      "Train Loss: 6844.191461103491\n",
      "Epoch 167\n",
      "Train Loss: 6844.132507047331\n",
      "Epoch 168\n",
      "Train Loss: 6844.07476661864\n",
      "Epoch 169\n",
      "Train Loss: 6844.018214832553\n",
      "Epoch 170\n",
      "Train Loss: 6843.962827218588\n",
      "Epoch 171\n",
      "Train Loss: 6843.908579810065\n",
      "Epoch 172\n",
      "Train Loss: 6843.855449133722\n",
      "Epoch 173\n",
      "Train Loss: 6843.803412199562\n",
      "Epoch 174\n",
      "Train Loss: 6843.752446490901\n",
      "Epoch 175\n",
      "Train Loss: 6843.702529954624\n",
      "Epoch 176\n",
      "Train Loss: 6843.653640991643\n",
      "Epoch 177\n",
      "Train Loss: 6843.605758447545\n",
      "Epoch 178\n",
      "Train Loss: 6843.558861603438\n",
      "Epoch 179\n",
      "Train Loss: 6843.512930166986\n",
      "Epoch 180\n",
      "Train Loss: 6843.467944263623\n",
      "Epoch 181\n",
      "Train Loss: 6843.42388442796\n",
      "Epoch 182\n",
      "Train Loss: 6843.3807315953445\n",
      "Epoch 183\n",
      "Train Loss: 6843.338467093628\n",
      "Epoch 184\n",
      "Train Loss: 6843.297072635076\n",
      "Epoch 185\n",
      "Train Loss: 6843.256530308453\n",
      "Epoch 186\n",
      "Train Loss: 6843.21682257127\n",
      "Epoch 187\n",
      "Train Loss: 6843.177932242198\n",
      "Epoch 188\n",
      "Train Loss: 6843.13984249363\n",
      "Epoch 189\n",
      "Train Loss: 6843.1025368443925\n",
      "Epoch 190\n",
      "Train Loss: 6843.065999152621\n",
      "Epoch 191\n",
      "Train Loss: 6843.030213608775\n",
      "Epoch 192\n",
      "Train Loss: 6842.995164728782\n",
      "Epoch 193\n",
      "Train Loss: 6842.960837347357\n",
      "Epoch 194\n",
      "Train Loss: 6842.927216611419\n",
      "Epoch 195\n",
      "Train Loss: 6842.894287973684\n",
      "Epoch 196\n",
      "Train Loss: 6842.862037186349\n",
      "Epoch 197\n",
      "Train Loss: 6842.830450294939\n",
      "Epoch 198\n",
      "Train Loss: 6842.799513632262\n",
      "Epoch 199\n",
      "Train Loss: 6842.7692138125\n",
      "Epoch 200\n",
      "Train Loss: 6842.739537725413\n",
      "Epoch 201\n",
      "Train Loss: 6842.710472530658\n",
      "Epoch 202\n",
      "Train Loss: 6842.682005652248\n",
      "Epoch 203\n",
      "Train Loss: 6842.654124773093\n",
      "Epoch 204\n",
      "Train Loss: 6842.626817829681\n",
      "Epoch 205\n",
      "Train Loss: 6842.600073006855\n",
      "Epoch 206\n",
      "Train Loss: 6842.5738787326945\n",
      "Epoch 207\n",
      "Train Loss: 6842.548223673513\n",
      "Epoch 208\n",
      "Train Loss: 6842.523096728954\n",
      "Epoch 209\n",
      "Train Loss: 6842.498487027183\n",
      "Epoch 210\n",
      "Train Loss: 6842.474383920184\n",
      "Epoch 211\n",
      "Train Loss: 6842.450776979154\n",
      "Epoch 212\n",
      "Train Loss: 6842.42765598999\n",
      "Epoch 213\n",
      "Train Loss: 6842.405010948859\n",
      "Epoch 214\n",
      "Train Loss: 6842.382832057886\n",
      "Epoch 215\n",
      "Train Loss: 6842.361109720899\n",
      "Epoch 216\n",
      "Train Loss: 6842.339834539283\n",
      "Epoch 217\n",
      "Train Loss: 6842.3189973079125\n",
      "Epoch 218\n",
      "Train Loss: 6842.298589011165\n",
      "Epoch 219\n",
      "Train Loss: 6842.278600819025\n",
      "Epoch 220\n",
      "Train Loss: 6842.259024083256\n",
      "Epoch 221\n",
      "Train Loss: 6842.239850333664\n",
      "Epoch 222\n",
      "Train Loss: 6842.221071274427\n",
      "Epoch 223\n",
      "Train Loss: 6842.202678780506\n",
      "Epoch 224\n",
      "Train Loss: 6842.184664894137\n",
      "Epoch 225\n",
      "Train Loss: 6842.167021821377\n",
      "Epoch 226\n",
      "Train Loss: 6842.149741928731\n",
      "Epoch 227\n",
      "Train Loss: 6842.132817739856\n",
      "Epoch 228\n",
      "Train Loss: 6842.116241932327\n",
      "Epoch 229\n",
      "Train Loss: 6842.100007334453\n",
      "Epoch 230\n",
      "Train Loss: 6842.084106922194\n",
      "Epoch 231\n",
      "Train Loss: 6842.068533816105\n",
      "Epoch 232\n",
      "Train Loss: 6842.0532812783695\n",
      "Epoch 233\n",
      "Train Loss: 6842.038342709878\n",
      "Epoch 234\n",
      "Train Loss: 6842.023711647371\n",
      "Epoch 235\n",
      "Train Loss: 6842.009381760651\n",
      "Epoch 236\n",
      "Train Loss: 6841.995346849835\n",
      "Epoch 237\n",
      "Train Loss: 6841.981600842675\n",
      "Epoch 238\n",
      "Train Loss: 6841.968137791926\n",
      "Epoch 239\n",
      "Train Loss: 6841.954951872777\n",
      "Epoch 240\n",
      "Train Loss: 6841.942037380334\n",
      "Epoch 241\n",
      "Train Loss: 6841.9293887271415\n",
      "Epoch 242\n",
      "Train Loss: 6841.917000440769\n",
      "Epoch 243\n",
      "Train Loss: 6841.904867161447\n",
      "Epoch 244\n",
      "Train Loss: 6841.892983639743\n",
      "Epoch 245\n",
      "Train Loss: 6841.881344734292\n",
      "Epoch 246\n",
      "Train Loss: 6841.869945409569\n",
      "Epoch 247\n",
      "Train Loss: 6841.858780733712\n",
      "Epoch 248\n",
      "Train Loss: 6841.84784587639\n",
      "Epoch 249\n",
      "Train Loss: 6841.837136106708\n",
      "Epoch 250\n",
      "Train Loss: 6841.826646791168\n",
      "Epoch 251\n",
      "Train Loss: 6841.816373391649\n",
      "Epoch 252\n",
      "Train Loss: 6841.806311463461\n",
      "Epoch 253\n",
      "Train Loss: 6841.796456653407\n",
      "Epoch 254\n",
      "Train Loss: 6841.786804697913\n",
      "Epoch 255\n",
      "Train Loss: 6841.777351421168\n",
      "Epoch 256\n",
      "Train Loss: 6841.7680927333295\n",
      "Epoch 257\n",
      "Train Loss: 6841.759024628751\n",
      "Epoch 258\n",
      "Train Loss: 6841.750143184237\n",
      "Epoch 259\n",
      "Train Loss: 6841.741444557366\n",
      "Epoch 260\n",
      "Train Loss: 6841.732924984814\n",
      "Epoch 261\n",
      "Train Loss: 6841.724580780725\n",
      "Epoch 262\n",
      "Train Loss: 6841.716408335124\n",
      "Epoch 263\n",
      "Train Loss: 6841.708404112348\n",
      "Epoch 264\n",
      "Train Loss: 6841.700564649523\n",
      "Epoch 265\n",
      "Train Loss: 6841.692886555057\n",
      "Epoch 266\n",
      "Train Loss: 6841.685366507182\n",
      "Epoch 267\n",
      "Train Loss: 6841.678001252509\n",
      "Epoch 268\n",
      "Train Loss: 6841.670787604621\n",
      "Epoch 269\n",
      "Train Loss: 6841.6637224427\n",
      "Epoch 270\n",
      "Train Loss: 6841.656802710171\n",
      "Epoch 271\n",
      "Train Loss: 6841.650025413382\n",
      "Epoch 272\n",
      "Train Loss: 6841.6433876203055\n",
      "Epoch 273\n",
      "Train Loss: 6841.636886459278\n",
      "Epoch 274\n",
      "Train Loss: 6841.630519117743\n",
      "Epoch 275\n",
      "Train Loss: 6841.624282841048\n",
      "Epoch 276\n",
      "Train Loss: 6841.618174931247\n",
      "Epoch 277\n",
      "Train Loss: 6841.612192745933\n",
      "Epoch 278\n",
      "Train Loss: 6841.606333697091\n",
      "Epoch 279\n",
      "Train Loss: 6841.60059524999\n",
      "Epoch 280\n",
      "Train Loss: 6841.594974922069\n",
      "Epoch 281\n",
      "Train Loss: 6841.589470281879\n",
      "Epoch 282\n",
      "Train Loss: 6841.584078948021\n",
      "Epoch 283\n",
      "Train Loss: 6841.578798588118\n",
      "Epoch 284\n",
      "Train Loss: 6841.57362691781\n",
      "Epoch 285\n",
      "Train Loss: 6841.568561699755\n",
      "Epoch 286\n",
      "Train Loss: 6841.563600742677\n",
      "Epoch 287\n",
      "Train Loss: 6841.558741900403\n",
      "Epoch 288\n",
      "Train Loss: 6841.553983070939\n",
      "Epoch 289\n",
      "Train Loss: 6841.549322195563\n",
      "Epoch 290\n",
      "Train Loss: 6841.544757257936\n",
      "Epoch 291\n",
      "Train Loss: 6841.540286283223\n",
      "Epoch 292\n",
      "Train Loss: 6841.535907337242\n",
      "Epoch 293\n",
      "Train Loss: 6841.531618525625\n",
      "Epoch 294\n",
      "Train Loss: 6841.5274179930075\n",
      "Epoch 295\n",
      "Train Loss: 6841.523303922211\n",
      "Epoch 296\n",
      "Train Loss: 6841.519274533466\n",
      "Epoch 297\n",
      "Train Loss: 6841.515328083644\n",
      "Epoch 298\n",
      "Train Loss: 6841.511462865495\n",
      "Epoch 299\n",
      "Train Loss: 6841.507677206913\n",
      "Epoch 300\n",
      "Train Loss: 6841.503969470217\n",
      "Epoch 301\n",
      "Train Loss: 6841.500338051436\n",
      "Epoch 302\n",
      "Train Loss: 6841.496781379615\n",
      "Epoch 303\n",
      "Train Loss: 6841.493297916139\n",
      "Epoch 304\n",
      "Train Loss: 6841.48988615407\n",
      "Epoch 305\n",
      "Train Loss: 6841.486544617483\n",
      "Epoch 306\n",
      "Train Loss: 6841.4832718608395\n",
      "Epoch 307\n",
      "Train Loss: 6841.480066468358\n",
      "Epoch 308\n",
      "Train Loss: 6841.476927053399\n",
      "Epoch 309\n",
      "Train Loss: 6841.473852257866\n",
      "Epoch 310\n",
      "Train Loss: 6841.470840751627\n",
      "Epoch 311\n",
      "Train Loss: 6841.467891231921\n",
      "Epoch 312\n",
      "Train Loss: 6841.465002422809\n",
      "Epoch 313\n",
      "Train Loss: 6841.4621730746185\n",
      "Epoch 314\n",
      "Train Loss: 6841.459401963398\n",
      "Epoch 315\n",
      "Train Loss: 6841.4566878903925\n",
      "Epoch 316\n",
      "Train Loss: 6841.454029681524\n",
      "Epoch 317\n",
      "Train Loss: 6841.451426186884\n",
      "Epoch 318\n",
      "Train Loss: 6841.448876280233\n",
      "Epoch 319\n",
      "Train Loss: 6841.446378858515\n",
      "Epoch 320\n",
      "Train Loss: 6841.44393284138\n",
      "Epoch 321\n",
      "Train Loss: 6841.441537170719\n",
      "Epoch 322\n",
      "Train Loss: 6841.4391908102025\n",
      "Epoch 323\n",
      "Train Loss: 6841.436892744834\n",
      "Epoch 324\n",
      "Train Loss: 6841.434641980508\n",
      "Epoch 325\n",
      "Train Loss: 6841.432437543588\n",
      "Epoch 326\n",
      "Train Loss: 6841.430278480476\n",
      "Epoch 327\n",
      "Train Loss: 6841.428163857203\n",
      "Epoch 328\n",
      "Train Loss: 6841.426092759027\n",
      "Epoch 329\n",
      "Train Loss: 6841.424064290034\n",
      "Epoch 330\n",
      "Train Loss: 6841.422077572757\n",
      "Epoch 331\n",
      "Train Loss: 6841.4201317477855\n",
      "Epoch 332\n",
      "Train Loss: 6841.4182259734025\n",
      "Epoch 333\n",
      "Train Loss: 6841.416359425215\n",
      "Epoch 334\n",
      "Train Loss: 6841.414531295805\n",
      "Epoch 335\n",
      "Train Loss: 6841.4127407943715\n",
      "Epoch 336\n",
      "Train Loss: 6841.410987146391\n",
      "Epoch 337\n",
      "Train Loss: 6841.409269593283\n",
      "Epoch 338\n",
      "Train Loss: 6841.4075873920865\n",
      "Epoch 339\n",
      "Train Loss: 6841.4059398151285\n",
      "Epoch 340\n",
      "Train Loss: 6841.404326149718\n",
      "Epoch 341\n",
      "Train Loss: 6841.402745697834\n",
      "Epoch 342\n",
      "Train Loss: 6841.401197775825\n",
      "Epoch 343\n",
      "Train Loss: 6841.399681714106\n",
      "Epoch 344\n",
      "Train Loss: 6841.398196856885\n",
      "Epoch 345\n",
      "Train Loss: 6841.396742561859\n",
      "Epoch 346\n",
      "Train Loss: 6841.395318199956\n",
      "Epoch 347\n",
      "Train Loss: 6841.393923155045\n",
      "Epoch 348\n",
      "Train Loss: 6841.39255682368\n",
      "Epoch 349\n",
      "Train Loss: 6841.391218614841\n",
      "Epoch 350\n",
      "Train Loss: 6841.389907949669\n",
      "Epoch 351\n",
      "Train Loss: 6841.38862426122\n",
      "Epoch 352\n",
      "Train Loss: 6841.387366994224\n",
      "Epoch 353\n",
      "Train Loss: 6841.386135604839\n",
      "Epoch 354\n",
      "Train Loss: 6841.384929560415\n",
      "Epoch 355\n",
      "Train Loss: 6841.383748339269\n",
      "Epoch 356\n",
      "Train Loss: 6841.382591430457\n",
      "Epoch 357\n",
      "Train Loss: 6841.381458333552\n",
      "Epoch 358\n",
      "Train Loss: 6841.380348558424\n",
      "Epoch 359\n",
      "Train Loss: 6841.3792616250375\n",
      "Epoch 360\n",
      "Train Loss: 6841.378197063235\n",
      "Epoch 361\n",
      "Train Loss: 6841.3771544125375\n",
      "Epoch 362\n",
      "Train Loss: 6841.376133221946\n",
      "Epoch 363\n",
      "Train Loss: 6841.375133049742\n",
      "Epoch 364\n",
      "Train Loss: 6841.374153463301\n",
      "Epoch 365\n",
      "Train Loss: 6841.373194038902\n",
      "Epoch 366\n",
      "Train Loss: 6841.3722543615495\n",
      "Epoch 367\n",
      "Train Loss: 6841.371334024788\n",
      "Epoch 368\n",
      "Train Loss: 6841.370432630528\n",
      "Epoch 369\n",
      "Train Loss: 6841.369549788875\n",
      "Epoch 370\n",
      "Train Loss: 6841.368685117959\n",
      "Epoch 371\n",
      "Train Loss: 6841.367838243774\n",
      "Epoch 372\n",
      "Train Loss: 6841.367008800009\n",
      "Epoch 373\n",
      "Train Loss: 6841.366196427892\n",
      "Epoch 374\n",
      "Train Loss: 6841.365400776039\n",
      "Epoch 375\n",
      "Train Loss: 6841.364621500298\n",
      "Epoch 376\n",
      "Train Loss: 6841.3638582636\n",
      "Epoch 377\n",
      "Train Loss: 6841.363110735817\n",
      "Epoch 378\n",
      "Train Loss: 6841.3623785936115\n",
      "Epoch 379\n",
      "Train Loss: 6841.361661520307\n",
      "Epoch 380\n",
      "Train Loss: 6841.360959205741\n",
      "Epoch 381\n",
      "Train Loss: 6841.360271346134\n",
      "Epoch 382\n",
      "Train Loss: 6841.359597643968\n",
      "Epoch 383\n",
      "Train Loss: 6841.35893780784\n",
      "Epoch 384\n",
      "Train Loss: 6841.358291552349\n",
      "Epoch 385\n",
      "Train Loss: 6841.357658597968\n",
      "Epoch 386\n",
      "Train Loss: 6841.357038670924\n",
      "Epoch 387\n",
      "Train Loss: 6841.35643150308\n",
      "Epoch 388\n",
      "Train Loss: 6841.355836831815\n",
      "Epoch 389\n",
      "Train Loss: 6841.355254399918\n",
      "Epoch 390\n",
      "Train Loss: 6841.35468395547\n",
      "Epoch 391\n",
      "Train Loss: 6841.354125251737\n",
      "Epoch 392\n",
      "Train Loss: 6841.353578047063\n",
      "Epoch 393\n",
      "Train Loss: 6841.35304210477\n",
      "Epoch 394\n",
      "Train Loss: 6841.35251719305\n",
      "Epoch 395\n",
      "Train Loss: 6841.352003084862\n",
      "Epoch 396\n",
      "Train Loss: 6841.3514995578425\n",
      "Epoch 397\n",
      "Train Loss: 6841.351006394207\n",
      "Epoch 398\n",
      "Train Loss: 6841.35052338065\n",
      "Epoch 399\n",
      "Train Loss: 6841.350050308256\n",
      "Epoch 400\n",
      "Train Loss: 6841.349586972413\n",
      "Epoch 401\n",
      "Train Loss: 6841.349133172718\n",
      "Epoch 402\n",
      "Train Loss: 6841.348688712895\n",
      "Epoch 403\n",
      "Train Loss: 6841.348253400705\n",
      "Epoch 404\n",
      "Train Loss: 6841.34782704787\n",
      "Epoch 405\n",
      "Train Loss: 6841.347409469984\n",
      "Epoch 406\n",
      "Train Loss: 6841.34700048644\n",
      "Epoch 407\n",
      "Train Loss: 6841.346599920344\n",
      "Epoch 408\n",
      "Train Loss: 6841.346207598446\n",
      "Epoch 409\n",
      "Train Loss: 6841.345823351061\n",
      "Epoch 410\n",
      "Train Loss: 6841.345447011998\n",
      "Epoch 411\n",
      "Train Loss: 6841.345078418486\n",
      "Epoch 412\n",
      "Train Loss: 6841.344717411103\n",
      "Epoch 413\n",
      "Train Loss: 6841.344363833712\n",
      "Epoch 414\n",
      "Train Loss: 6841.344017533386\n",
      "Epoch 415\n",
      "Train Loss: 6841.343678360346\n",
      "Epoch 416\n",
      "Train Loss: 6841.3433461679\n",
      "Epoch 417\n",
      "Train Loss: 6841.34302081237\n",
      "Epoch 418\n",
      "Train Loss: 6841.342702153037\n",
      "Epoch 419\n",
      "Train Loss: 6841.34239005208\n",
      "Epoch 420\n",
      "Train Loss: 6841.342084374518\n",
      "Epoch 421\n",
      "Train Loss: 6841.341784988139\n",
      "Epoch 422\n",
      "Train Loss: 6841.341491763458\n",
      "Epoch 423\n",
      "Train Loss: 6841.3412045736595\n",
      "Epoch 424\n",
      "Train Loss: 6841.340923294528\n",
      "Epoch 425\n",
      "Train Loss: 6841.340647804413\n",
      "Epoch 426\n",
      "Train Loss: 6841.340377984166\n",
      "Epoch 427\n",
      "Train Loss: 6841.340113717085\n",
      "Epoch 428\n",
      "Train Loss: 6841.339854888883\n",
      "Epoch 429\n",
      "Train Loss: 6841.339601387611\n",
      "Epoch 430\n",
      "Train Loss: 6841.339353103633\n",
      "Epoch 431\n",
      "Train Loss: 6841.339109929568\n",
      "Epoch 432\n",
      "Train Loss: 6841.338871760244\n",
      "Epoch 433\n",
      "Train Loss: 6841.338638492653\n",
      "Epoch 434\n",
      "Train Loss: 6841.338410025911\n",
      "Epoch 435\n",
      "Train Loss: 6841.338186261206\n",
      "Epoch 436\n",
      "Train Loss: 6841.337967101763\n",
      "Epoch 437\n",
      "Train Loss: 6841.337752452794\n",
      "Epoch 438\n",
      "Train Loss: 6841.337542221471\n",
      "Epoch 439\n",
      "Train Loss: 6841.337336316869\n",
      "Epoch 440\n",
      "Train Loss: 6841.337134649936\n",
      "Epoch 441\n",
      "Train Loss: 6841.336937133454\n",
      "Epoch 442\n",
      "Train Loss: 6841.336743682\n",
      "Epoch 443\n",
      "Train Loss: 6841.336554211909\n",
      "Epoch 444\n",
      "Train Loss: 6841.336368641238\n",
      "Epoch 445\n",
      "Train Loss: 6841.336186889732\n",
      "Epoch 446\n",
      "Train Loss: 6841.336008878783\n",
      "Epoch 447\n",
      "Train Loss: 6841.335834531407\n",
      "Epoch 448\n",
      "Train Loss: 6841.3356637722045\n",
      "Epoch 449\n",
      "Train Loss: 6841.33549652732\n",
      "Epoch 450\n",
      "Train Loss: 6841.3353327244295\n",
      "Epoch 451\n",
      "Train Loss: 6841.335172292687\n",
      "Epoch 452\n",
      "Train Loss: 6841.33501516271\n",
      "Epoch 453\n",
      "Train Loss: 6841.334861266545\n",
      "Epoch 454\n",
      "Train Loss: 6841.334710537634\n",
      "Epoch 455\n",
      "Train Loss: 6841.334562910793\n",
      "Epoch 456\n",
      "Train Loss: 6841.334418322174\n",
      "Epoch 457\n",
      "Train Loss: 6841.334276709247\n",
      "Epoch 458\n",
      "Train Loss: 6841.33413801077\n",
      "Epoch 459\n",
      "Train Loss: 6841.334002166759\n",
      "Epoch 460\n",
      "Train Loss: 6841.3338691184645\n",
      "Epoch 461\n",
      "Train Loss: 6841.333738808346\n",
      "Epoch 462\n",
      "Train Loss: 6841.333611180051\n",
      "Epoch 463\n",
      "Train Loss: 6841.333486178384\n",
      "Epoch 464\n",
      "Train Loss: 6841.333363749283\n",
      "Epoch 465\n",
      "Train Loss: 6841.333243839804\n",
      "Epoch 466\n",
      "Train Loss: 6841.333126398091\n",
      "Epoch 467\n",
      "Train Loss: 6841.333011373351\n",
      "Epoch 468\n",
      "Train Loss: 6841.332898715842\n",
      "Epoch 469\n",
      "Train Loss: 6841.332788376843\n",
      "Epoch 470\n",
      "Train Loss: 6841.33268030864\n",
      "Epoch 471\n",
      "Train Loss: 6841.332574464492\n",
      "Epoch 472\n",
      "Train Loss: 6841.332470798628\n",
      "Epoch 473\n",
      "Train Loss: 6841.332369266218\n",
      "Epoch 474\n",
      "Train Loss: 6841.332269823352\n",
      "Epoch 475\n",
      "Train Loss: 6841.3321724270245\n",
      "Epoch 476\n",
      "Train Loss: 6841.332077035116\n",
      "Epoch 477\n",
      "Train Loss: 6841.331983606374\n",
      "Epoch 478\n",
      "Train Loss: 6841.331892100395\n",
      "Epoch 479\n",
      "Train Loss: 6841.331802477605\n",
      "Epoch 480\n",
      "Train Loss: 6841.331714699249\n",
      "Epoch 481\n",
      "Train Loss: 6841.331628727365\n",
      "Epoch 482\n",
      "Train Loss: 6841.3315445247745\n",
      "Epoch 483\n",
      "Train Loss: 6841.331462055064\n",
      "Epoch 484\n",
      "Train Loss: 6841.331381282571\n",
      "Epoch 485\n",
      "Train Loss: 6841.331302172361\n",
      "Epoch 486\n",
      "Train Loss: 6841.331224690226\n",
      "Epoch 487\n",
      "Train Loss: 6841.331148802661\n",
      "Epoch 488\n",
      "Train Loss: 6841.331074476844\n",
      "Epoch 489\n",
      "Train Loss: 6841.331001680635\n",
      "Epoch 490\n",
      "Train Loss: 6841.330930382556\n",
      "Epoch 491\n",
      "Train Loss: 6841.33086055177\n",
      "Epoch 492\n",
      "Train Loss: 6841.330792158082\n",
      "Epoch 493\n",
      "Train Loss: 6841.330725171916\n",
      "Epoch 494\n",
      "Train Loss: 6841.330659564303\n",
      "Epoch 495\n",
      "Train Loss: 6841.330595306871\n",
      "Epoch 496\n",
      "Train Loss: 6841.330532371831\n",
      "Epoch 497\n",
      "Train Loss: 6841.330470731973\n",
      "Epoch 498\n",
      "Train Loss: 6841.330410360636\n",
      "Epoch 499\n",
      "Train Loss: 6841.330351231716\n",
      "Epoch 500\n",
      "Train Loss: 6841.330293319642\n",
      "Epoch 501\n",
      "Train Loss: 6841.33023659937\n",
      "Epoch 502\n",
      "Train Loss: 6841.330181046375\n",
      "Epoch 503\n",
      "Train Loss: 6841.330126636633\n",
      "Epoch 504\n",
      "Train Loss: 6841.330073346613\n",
      "Epoch 505\n",
      "Train Loss: 6841.330021153273\n",
      "Epoch 506\n",
      "Train Loss: 6841.329970034042\n",
      "Epoch 507\n",
      "Train Loss: 6841.3299199668145\n",
      "Epoch 508\n",
      "Train Loss: 6841.3298709299415\n",
      "Epoch 509\n",
      "Train Loss: 6841.329822902214\n",
      "Epoch 510\n",
      "Train Loss: 6841.329775862866\n",
      "Epoch 511\n",
      "Train Loss: 6841.329729791557\n",
      "Epoch 512\n",
      "Train Loss: 6841.3296846683625\n",
      "Epoch 513\n",
      "Train Loss: 6841.329640473772\n",
      "Epoch 514\n",
      "Train Loss: 6841.329597188675\n",
      "Epoch 515\n",
      "Train Loss: 6841.329554794351\n",
      "Epoch 516\n",
      "Train Loss: 6841.329513272469\n",
      "Epoch 517\n",
      "Train Loss: 6841.329472605074\n",
      "Epoch 518\n",
      "Train Loss: 6841.329432774583\n",
      "Epoch 519\n",
      "Train Loss: 6841.329393763771\n",
      "Epoch 520\n",
      "Train Loss: 6841.329355555766\n",
      "Epoch 521\n",
      "Train Loss: 6841.329318134048\n",
      "Epoch 522\n",
      "Train Loss: 6841.329281482436\n",
      "Epoch 523\n",
      "Train Loss: 6841.3292455850815\n",
      "Epoch 524\n",
      "Train Loss: 6841.32921042646\n",
      "Epoch 525\n",
      "Train Loss: 6841.329175991371\n",
      "Epoch 526\n",
      "Train Loss: 6841.32914226492\n",
      "Epoch 527\n",
      "Train Loss: 6841.329109232528\n",
      "Epoch 528\n",
      "Train Loss: 6841.329076879907\n",
      "Epoch 529\n",
      "Train Loss: 6841.329045193072\n",
      "Epoch 530\n",
      "Train Loss: 6841.329014158316\n",
      "Epoch 531\n",
      "Train Loss: 6841.328983762222\n",
      "Epoch 532\n",
      "Train Loss: 6841.328953991648\n",
      "Epoch 533\n",
      "Train Loss: 6841.328924833718\n",
      "Epoch 534\n",
      "Train Loss: 6841.328896275825\n",
      "Epoch 535\n",
      "Train Loss: 6841.32886830562\n",
      "Epoch 536\n",
      "Train Loss: 6841.328840911009\n",
      "Epoch 537\n",
      "Train Loss: 6841.328814080147\n",
      "Epoch 538\n",
      "Train Loss: 6841.328787801431\n",
      "Epoch 539\n",
      "Train Loss: 6841.328762063497\n",
      "Epoch 540\n",
      "Train Loss: 6841.328736855219\n",
      "Epoch 541\n",
      "Train Loss: 6841.328712165696\n",
      "Epoch 542\n",
      "Train Loss: 6841.328687984248\n",
      "Epoch 543\n",
      "Train Loss: 6841.328664300424\n",
      "Epoch 544\n",
      "Train Loss: 6841.328641103982\n",
      "Epoch 545\n",
      "Train Loss: 6841.32861838489\n",
      "Epoch 546\n",
      "Train Loss: 6841.328596133326\n",
      "Epoch 547\n",
      "Train Loss: 6841.328574339667\n",
      "Epoch 548\n",
      "Train Loss: 6841.3285529944915\n",
      "Epoch 549\n",
      "Train Loss: 6841.328532088569\n",
      "Epoch 550\n",
      "Train Loss: 6841.328511612858\n",
      "Epoch 551\n",
      "Train Loss: 6841.328491558508\n",
      "Epoch 552\n",
      "Train Loss: 6841.328471916847\n",
      "Epoch 553\n",
      "Train Loss: 6841.3284526793805\n",
      "Epoch 554\n",
      "Train Loss: 6841.328433837791\n",
      "Epoch 555\n",
      "Train Loss: 6841.328415383932\n",
      "Epoch 556\n",
      "Train Loss: 6841.328397309824\n",
      "Epoch 557\n",
      "Train Loss: 6841.328379607652\n",
      "Epoch 558\n",
      "Train Loss: 6841.328362269764\n",
      "Epoch 559\n",
      "Train Loss: 6841.328345288659\n",
      "Epoch 560\n",
      "Train Loss: 6841.328328656996\n",
      "Epoch 561\n",
      "Train Loss: 6841.3283123675865\n",
      "Epoch 562\n",
      "Train Loss: 6841.328296413384\n",
      "Epoch 563\n",
      "Train Loss: 6841.328280787492\n",
      "Epoch 564\n",
      "Train Loss: 6841.328265483151\n",
      "Epoch 565\n",
      "Train Loss: 6841.32825049375\n",
      "Epoch 566\n",
      "Train Loss: 6841.328235812801\n",
      "Epoch 567\n",
      "Train Loss: 6841.3282214339615\n",
      "Epoch 568\n",
      "Train Loss: 6841.328207351011\n",
      "Epoch 569\n",
      "Train Loss: 6841.328193557862\n",
      "Epoch 570\n",
      "Train Loss: 6841.328180048548\n",
      "Epoch 571\n",
      "Train Loss: 6841.328166817231\n",
      "Epoch 572\n",
      "Train Loss: 6841.328153858191\n",
      "Epoch 573\n",
      "Train Loss: 6841.328141165821\n",
      "Epoch 574\n",
      "Train Loss: 6841.328128734635\n",
      "Epoch 575\n",
      "Train Loss: 6841.328116559258\n",
      "Epoch 576\n",
      "Train Loss: 6841.328104634427\n",
      "Epoch 577\n",
      "Train Loss: 6841.328092954986\n",
      "Epoch 578\n",
      "Train Loss: 6841.328081515881\n",
      "Epoch 579\n",
      "Train Loss: 6841.328070312172\n",
      "Epoch 580\n",
      "Train Loss: 6841.328059339011\n",
      "Epoch 581\n",
      "Train Loss: 6841.328048591655\n",
      "Epoch 582\n",
      "Train Loss: 6841.328038065456\n",
      "Epoch 583\n",
      "Train Loss: 6841.3280277558615\n",
      "Epoch 584\n",
      "Train Loss: 6841.3280176584185\n",
      "Epoch 585\n",
      "Train Loss: 6841.328007768759\n",
      "Epoch 586\n",
      "Train Loss: 6841.3279980826055\n",
      "Epoch 587\n",
      "Train Loss: 6841.327988595773\n",
      "Epoch 588\n",
      "Train Loss: 6841.327979304157\n",
      "Epoch 589\n",
      "Train Loss: 6841.327970203741\n",
      "Epoch 590\n",
      "Train Loss: 6841.327961290591\n",
      "Epoch 591\n",
      "Train Loss: 6841.327952560855\n",
      "Epoch 592\n",
      "Train Loss: 6841.3279440107535\n",
      "Epoch 593\n",
      "Train Loss: 6841.327935636596\n",
      "Epoch 594\n",
      "Train Loss: 6841.327927434756\n",
      "Epoch 595\n",
      "Train Loss: 6841.327919401692\n",
      "Epoch 596\n",
      "Train Loss: 6841.327911533927\n",
      "Epoch 597\n",
      "Train Loss: 6841.327903828063\n",
      "Epoch 598\n",
      "Train Loss: 6841.327896280765\n",
      "Epoch 599\n",
      "Train Loss: 6841.3278888887735\n",
      "Epoch 600\n",
      "Train Loss: 6841.327881648892\n",
      "Epoch 601\n",
      "Train Loss: 6841.327874557986\n",
      "Epoch 602\n",
      "Train Loss: 6841.327867612995\n",
      "Epoch 603\n",
      "Train Loss: 6841.327860810913\n",
      "Epoch 604\n",
      "Train Loss: 6841.327854148801\n",
      "Epoch 605\n",
      "Train Loss: 6841.3278476237765\n",
      "Epoch 606\n",
      "Train Loss: 6841.327841233021\n",
      "Epoch 607\n",
      "Train Loss: 6841.32783497377\n",
      "Epoch 608\n",
      "Train Loss: 6841.327828843318\n",
      "Epoch 609\n",
      "Train Loss: 6841.3278228390145\n",
      "Epoch 610\n",
      "Train Loss: 6841.327816958262\n",
      "Epoch 611\n",
      "Train Loss: 6841.327811198519\n",
      "Epoch 612\n",
      "Train Loss: 6841.327805557297\n",
      "Epoch 613\n",
      "Train Loss: 6841.327800032157\n",
      "Epoch 614\n",
      "Train Loss: 6841.327794620706\n",
      "Epoch 615\n",
      "Train Loss: 6841.327789320609\n",
      "Epoch 616\n",
      "Train Loss: 6841.327784129572\n",
      "Epoch 617\n",
      "Train Loss: 6841.327779045353\n",
      "Epoch 618\n",
      "Train Loss: 6841.327774065751\n",
      "Epoch 619\n",
      "Train Loss: 6841.327769188616\n",
      "Epoch 620\n",
      "Train Loss: 6841.327764411837\n",
      "Epoch 621\n",
      "Train Loss: 6841.32775973335\n",
      "Epoch 622\n",
      "Train Loss: 6841.327755151134\n",
      "Epoch 623\n",
      "Train Loss: 6841.327750663207\n",
      "Epoch 624\n",
      "Train Loss: 6841.327746267625\n",
      "Epoch 625\n",
      "Train Loss: 6841.327741962494\n",
      "Epoch 626\n",
      "Train Loss: 6841.327737745947\n",
      "Epoch 627\n",
      "Train Loss: 6841.327733616165\n",
      "Epoch 628\n",
      "Train Loss: 6841.32772957136\n",
      "Epoch 629\n",
      "Train Loss: 6841.327725609785\n",
      "Epoch 630\n",
      "Train Loss: 6841.327721729726\n",
      "Epoch 631\n",
      "Train Loss: 6841.327717929508\n",
      "Epoch 632\n",
      "Train Loss: 6841.327714207486\n",
      "Epoch 633\n",
      "Train Loss: 6841.327710562051\n",
      "Epoch 634\n",
      "Train Loss: 6841.327706991627\n",
      "Epoch 635\n",
      "Train Loss: 6841.327703494671\n",
      "Epoch 636\n",
      "Train Loss: 6841.327700069671\n",
      "Epoch 637\n",
      "Train Loss: 6841.327696715145\n",
      "Epoch 638\n",
      "Train Loss: 6841.327693429646\n",
      "Epoch 639\n",
      "Train Loss: 6841.32769021175\n",
      "Epoch 640\n",
      "Train Loss: 6841.327687060069\n",
      "Epoch 641\n",
      "Train Loss: 6841.327683973238\n",
      "Epoch 642\n",
      "Train Loss: 6841.327680949924\n",
      "Epoch 643\n",
      "Train Loss: 6841.327677988819\n",
      "Epoch 644\n",
      "Train Loss: 6841.327675088645\n",
      "Epoch 645\n",
      "Train Loss: 6841.327672248144\n",
      "Epoch 646\n",
      "Train Loss: 6841.327669466092\n",
      "Epoch 647\n",
      "Train Loss: 6841.327666741286\n",
      "Epoch 648\n",
      "Train Loss: 6841.327664072545\n",
      "Epoch 649\n",
      "Train Loss: 6841.327661458718\n",
      "Epoch 650\n",
      "Train Loss: 6841.327658898675\n",
      "Epoch 651\n",
      "Train Loss: 6841.327656391308\n",
      "Epoch 652\n",
      "Train Loss: 6841.327653935533\n",
      "Epoch 653\n",
      "Train Loss: 6841.32765153029\n",
      "Epoch 654\n",
      "Train Loss: 6841.3276491745355\n",
      "Epoch 655\n",
      "Train Loss: 6841.327646867256\n",
      "Epoch 656\n",
      "Train Loss: 6841.3276446074515\n",
      "Epoch 657\n",
      "Train Loss: 6841.327642394146\n",
      "Epoch 658\n",
      "Train Loss: 6841.327640226381\n",
      "Epoch 659\n",
      "Train Loss: 6841.32763810322\n",
      "Epoch 660\n",
      "Train Loss: 6841.327636023748\n",
      "Epoch 661\n",
      "Train Loss: 6841.327633987063\n",
      "Epoch 662\n",
      "Train Loss: 6841.327631992283\n",
      "Epoch 663\n",
      "Train Loss: 6841.327630038549\n",
      "Epoch 664\n",
      "Train Loss: 6841.3276281250155\n",
      "Epoch 665\n",
      "Train Loss: 6841.327626250855\n",
      "Epoch 666\n",
      "Train Loss: 6841.3276244152585\n",
      "Epoch 667\n",
      "Train Loss: 6841.32762261743\n",
      "Epoch 668\n",
      "Train Loss: 6841.327620856595\n",
      "Epoch 669\n",
      "Train Loss: 6841.327619131989\n",
      "Epoch 670\n",
      "Train Loss: 6841.32761744287\n",
      "Epoch 671\n",
      "Train Loss: 6841.327615788506\n",
      "Epoch 672\n",
      "Train Loss: 6841.327614168183\n",
      "Epoch 673\n",
      "Train Loss: 6841.327612581197\n",
      "Epoch 674\n",
      "Train Loss: 6841.327611026867\n",
      "Epoch 675\n",
      "Train Loss: 6841.3276095045185\n",
      "Epoch 676\n",
      "Train Loss: 6841.327608013493\n",
      "Epoch 677\n",
      "Train Loss: 6841.327606553149\n",
      "Epoch 678\n",
      "Train Loss: 6841.3276051228495\n",
      "Epoch 679\n",
      "Train Loss: 6841.32760372198\n",
      "Epoch 680\n",
      "Train Loss: 6841.3276023499375\n",
      "Epoch 681\n",
      "Train Loss: 6841.327601006122\n",
      "Epoch 682\n",
      "Train Loss: 6841.327599689958\n",
      "Epoch 683\n",
      "Train Loss: 6841.327598400877\n",
      "Epoch 684\n",
      "Train Loss: 6841.327597138316\n",
      "Epoch 685\n",
      "Train Loss: 6841.327595901736\n",
      "Epoch 686\n",
      "Train Loss: 6841.327594690599\n",
      "Epoch 687\n",
      "Train Loss: 6841.32759350438\n",
      "Epoch 688\n",
      "Train Loss: 6841.32759234257\n",
      "Epoch 689\n",
      "Train Loss: 6841.327591204665\n",
      "Epoch 690\n",
      "Train Loss: 6841.327590090172\n",
      "Epoch 691\n",
      "Train Loss: 6841.327588998612\n",
      "Epoch 692\n",
      "Train Loss: 6841.32758792951\n",
      "Epoch 693\n",
      "Train Loss: 6841.327586882406\n",
      "Epoch 694\n",
      "Train Loss: 6841.327585856847\n",
      "Epoch 695\n",
      "Train Loss: 6841.327584852387\n",
      "Epoch 696\n",
      "Train Loss: 6841.327583868596\n",
      "Epoch 697\n",
      "Train Loss: 6841.327582905047\n",
      "Epoch 698\n",
      "Train Loss: 6841.327581961324\n",
      "Epoch 699\n",
      "Train Loss: 6841.327581037017\n",
      "Epoch 700\n",
      "Train Loss: 6841.327580131729\n",
      "Epoch 701\n",
      "Train Loss: 6841.327579245067\n",
      "Epoch 702\n",
      "Train Loss: 6841.327578376649\n",
      "Epoch 703\n",
      "Train Loss: 6841.327577526098\n",
      "Epoch 704\n",
      "Train Loss: 6841.327576693049\n",
      "Epoch 705\n",
      "Train Loss: 6841.327575877139\n",
      "Epoch 706\n",
      "Train Loss: 6841.327575078016\n",
      "Epoch 707\n",
      "Train Loss: 6841.327574295336\n",
      "Epoch 708\n",
      "Train Loss: 6841.327573528759\n",
      "Epoch 709\n",
      "Train Loss: 6841.3275727779555\n",
      "Epoch 710\n",
      "Train Loss: 6841.327572042599\n",
      "Epoch 711\n",
      "Train Loss: 6841.327571322373\n",
      "Epoch 712\n",
      "Train Loss: 6841.327570616965\n",
      "Epoch 713\n",
      "Train Loss: 6841.32756992607\n",
      "Epoch 714\n",
      "Train Loss: 6841.327569249393\n",
      "Epoch 715\n",
      "Train Loss: 6841.327568586637\n",
      "Epoch 716\n",
      "Train Loss: 6841.327567937518\n",
      "Epoch 717\n",
      "Train Loss: 6841.327567301753\n",
      "Epoch 718\n",
      "Train Loss: 6841.327566679071\n",
      "Epoch 719\n",
      "Train Loss: 6841.3275660692\n",
      "Epoch 720\n",
      "Train Loss: 6841.327565471876\n",
      "Epoch 721\n",
      "Train Loss: 6841.327564886841\n",
      "Epoch 722\n",
      "Train Loss: 6841.327564313846\n",
      "Epoch 723\n",
      "Train Loss: 6841.3275637526385\n",
      "Epoch 724\n",
      "Train Loss: 6841.327563202978\n",
      "Epoch 725\n",
      "Train Loss: 6841.327562664627\n",
      "Epoch 726\n",
      "Train Loss: 6841.327562137351\n",
      "Epoch 727\n",
      "Train Loss: 6841.327561620926\n",
      "Epoch 728\n",
      "Train Loss: 6841.327561115125\n",
      "Epoch 729\n",
      "Train Loss: 6841.327560619729\n",
      "Epoch 730\n",
      "Train Loss: 6841.3275601345285\n",
      "Epoch 731\n",
      "Train Loss: 6841.3275596593085\n",
      "Epoch 732\n",
      "Train Loss: 6841.327559193868\n",
      "Epoch 733\n",
      "Train Loss: 6841.327558738003\n",
      "Epoch 734\n",
      "Train Loss: 6841.3275582915185\n",
      "Epoch 735\n",
      "Train Loss: 6841.327557854219\n",
      "Epoch 736\n",
      "Train Loss: 6841.327557425916\n",
      "Epoch 737\n",
      "Train Loss: 6841.327557006425\n",
      "Epoch 738\n",
      "Train Loss: 6841.327556595566\n",
      "Epoch 739\n",
      "Train Loss: 6841.3275561931605\n",
      "Epoch 740\n",
      "Train Loss: 6841.327555799034\n",
      "Epoch 741\n",
      "Train Loss: 6841.327555413015\n",
      "Epoch 742\n",
      "Train Loss: 6841.32755503494\n",
      "Epoch 743\n",
      "Train Loss: 6841.327554664641\n",
      "Epoch 744\n",
      "Train Loss: 6841.327554301964\n",
      "Epoch 745\n",
      "Train Loss: 6841.327553946747\n",
      "Epoch 746\n",
      "Train Loss: 6841.32755359884\n",
      "Epoch 747\n",
      "Train Loss: 6841.327553258089\n",
      "Epoch 748\n",
      "Train Loss: 6841.32755292435\n",
      "Epoch 749\n",
      "Train Loss: 6841.327552597476\n",
      "Epoch 750\n",
      "Train Loss: 6841.327552277329\n",
      "Epoch 751\n",
      "Train Loss: 6841.327551963767\n",
      "Epoch 752\n",
      "Train Loss: 6841.327551656657\n",
      "Epoch 753\n",
      "Train Loss: 6841.327551355865\n",
      "Epoch 754\n",
      "Train Loss: 6841.327551061262\n",
      "Epoch 755\n",
      "Train Loss: 6841.327550772721\n",
      "Epoch 756\n",
      "Train Loss: 6841.327550490117\n",
      "Epoch 757\n",
      "Train Loss: 6841.327550213325\n",
      "Epoch 758\n",
      "Train Loss: 6841.327549942229\n",
      "Epoch 759\n",
      "Train Loss: 6841.327549676712\n",
      "Epoch 760\n",
      "Train Loss: 6841.327549416655\n",
      "Epoch 761\n",
      "Train Loss: 6841.32754916195\n",
      "Epoch 762\n",
      "Train Loss: 6841.327548912486\n",
      "Epoch 763\n",
      "Train Loss: 6841.327548668152\n",
      "Epoch 764\n",
      "Train Loss: 6841.327548428846\n",
      "Epoch 765\n",
      "Train Loss: 6841.327548194464\n",
      "Epoch 766\n",
      "Train Loss: 6841.327547964904\n",
      "Epoch 767\n",
      "Train Loss: 6841.327547740066\n",
      "Epoch 768\n",
      "Train Loss: 6841.327547519855\n",
      "Epoch 769\n",
      "Train Loss: 6841.327547304174\n",
      "Epoch 770\n",
      "Train Loss: 6841.32754709293\n",
      "Epoch 771\n",
      "Train Loss: 6841.327546886035\n",
      "Epoch 772\n",
      "Train Loss: 6841.327546683394\n",
      "Epoch 773\n",
      "Train Loss: 6841.327546484921\n",
      "Epoch 774\n",
      "Train Loss: 6841.327546290533\n",
      "Epoch 775\n",
      "Train Loss: 6841.327546100144\n",
      "Epoch 776\n",
      "Train Loss: 6841.327545913672\n",
      "Epoch 777\n",
      "Train Loss: 6841.327545731037\n",
      "Epoch 778\n",
      "Train Loss: 6841.327545552159\n",
      "Epoch 779\n",
      "Train Loss: 6841.327545376962\n",
      "Epoch 780\n",
      "Train Loss: 6841.327545205368\n",
      "Epoch 781\n",
      "Train Loss: 6841.327545037304\n",
      "Epoch 782\n",
      "Train Loss: 6841.327544872697\n",
      "Epoch 783\n",
      "Train Loss: 6841.327544711479\n",
      "Epoch 784\n",
      "Train Loss: 6841.327544553576\n",
      "Epoch 785\n",
      "Train Loss: 6841.327544398923\n",
      "Epoch 786\n",
      "Train Loss: 6841.3275442474505\n",
      "Epoch 787\n",
      "Train Loss: 6841.327544099096\n",
      "Epoch 788\n",
      "Train Loss: 6841.327543953793\n",
      "Epoch 789\n",
      "Train Loss: 6841.327543811478\n",
      "Epoch 790\n",
      "Train Loss: 6841.327543672092\n",
      "Epoch 791\n",
      "Train Loss: 6841.327543535574\n",
      "Epoch 792\n",
      "Train Loss: 6841.327543401863\n",
      "Epoch 793\n",
      "Train Loss: 6841.327543270904\n",
      "Epoch 794\n",
      "Train Loss: 6841.32754314264\n",
      "Epoch 795\n",
      "Train Loss: 6841.327543017013\n",
      "Epoch 796\n",
      "Train Loss: 6841.327542893973\n",
      "Epoch 797\n",
      "Train Loss: 6841.3275427734625\n",
      "Epoch 798\n",
      "Train Loss: 6841.327542655431\n",
      "Epoch 799\n",
      "Train Loss: 6841.327542539829\n",
      "Epoch 800\n",
      "Train Loss: 6841.327542426606\n",
      "Epoch 801\n",
      "Train Loss: 6841.32754231571\n",
      "Epoch 802\n",
      "Train Loss: 6841.3275422070965\n",
      "Epoch 803\n",
      "Train Loss: 6841.327542100718\n",
      "Epoch 804\n",
      "Train Loss: 6841.327541996528\n",
      "Epoch 805\n",
      "Train Loss: 6841.32754189448\n",
      "Epoch 806\n",
      "Train Loss: 6841.327541794533\n",
      "Epoch 807\n",
      "Train Loss: 6841.327541696641\n",
      "Epoch 808\n",
      "Train Loss: 6841.327541600766\n",
      "Epoch 809\n",
      "Train Loss: 6841.327541506859\n",
      "Epoch 810\n",
      "Train Loss: 6841.327541414887\n",
      "Epoch 811\n",
      "Train Loss: 6841.327541324805\n",
      "Epoch 812\n",
      "Train Loss: 6841.327541236578\n",
      "Epoch 813\n",
      "Train Loss: 6841.327541150165\n",
      "Epoch 814\n",
      "Train Loss: 6841.3275410655315\n",
      "Epoch 815\n",
      "Train Loss: 6841.3275409826365\n",
      "Epoch 816\n",
      "Train Loss: 6841.327540901449\n",
      "Epoch 817\n",
      "Train Loss: 6841.327540821931\n",
      "Epoch 818\n",
      "Train Loss: 6841.32754074405\n",
      "Epoch 819\n",
      "Train Loss: 6841.32754066777\n",
      "Epoch 820\n",
      "Train Loss: 6841.327540593059\n",
      "Epoch 821\n",
      "Train Loss: 6841.327540519886\n",
      "Epoch 822\n",
      "Train Loss: 6841.327540448217\n",
      "Epoch 823\n",
      "Train Loss: 6841.327540378023\n",
      "Epoch 824\n",
      "Train Loss: 6841.327540309275\n",
      "Epoch 825\n",
      "Train Loss: 6841.327540241939\n",
      "Epoch 826\n",
      "Train Loss: 6841.327540175988\n",
      "Epoch 827\n",
      "Train Loss: 6841.327540111395\n",
      "Epoch 828\n",
      "Train Loss: 6841.327540048131\n",
      "Epoch 829\n",
      "Train Loss: 6841.327539986168\n",
      "Epoch 830\n",
      "Train Loss: 6841.32753992548\n",
      "Epoch 831\n",
      "Train Loss: 6841.32753986604\n",
      "Epoch 832\n",
      "Train Loss: 6841.327539807823\n",
      "Epoch 833\n",
      "Train Loss: 6841.327539750803\n",
      "Epoch 834\n",
      "Train Loss: 6841.327539694957\n",
      "Epoch 835\n",
      "Train Loss: 6841.32753964026\n",
      "Epoch 836\n",
      "Train Loss: 6841.327539586688\n",
      "Epoch 837\n",
      "Train Loss: 6841.327539534217\n",
      "Epoch 838\n",
      "Train Loss: 6841.327539482828\n",
      "Epoch 839\n",
      "Train Loss: 6841.327539432493\n",
      "Epoch 840\n",
      "Train Loss: 6841.327539383196\n",
      "Epoch 841\n",
      "Train Loss: 6841.327539334912\n",
      "Epoch 842\n",
      "Train Loss: 6841.327539287621\n",
      "Epoch 843\n",
      "Train Loss: 6841.327539241304\n",
      "Epoch 844\n",
      "Train Loss: 6841.3275391959405\n",
      "Epoch 845\n",
      "Train Loss: 6841.327539151508\n",
      "Epoch 846\n",
      "Train Loss: 6841.327539107991\n",
      "Epoch 847\n",
      "Train Loss: 6841.327539065368\n",
      "Epoch 848\n",
      "Train Loss: 6841.327539023623\n",
      "Epoch 849\n",
      "Train Loss: 6841.327538982737\n",
      "Epoch 850\n",
      "Train Loss: 6841.327538942691\n",
      "Epoch 851\n",
      "Train Loss: 6841.32753890347\n",
      "Epoch 852\n",
      "Train Loss: 6841.327538865055\n",
      "Epoch 853\n",
      "Train Loss: 6841.32753882743\n",
      "Epoch 854\n",
      "Train Loss: 6841.327538790579\n",
      "Epoch 855\n",
      "Train Loss: 6841.327538754488\n",
      "Epoch 856\n",
      "Train Loss: 6841.327538719137\n",
      "Epoch 857\n",
      "Train Loss: 6841.327538684514\n",
      "Epoch 858\n",
      "Train Loss: 6841.327538650604\n",
      "Epoch 859\n",
      "Train Loss: 6841.327538617392\n",
      "Epoch 860\n",
      "Train Loss: 6841.327538584861\n",
      "Epoch 861\n",
      "Train Loss: 6841.327538553001\n",
      "Epoch 862\n",
      "Train Loss: 6841.327538521796\n",
      "Epoch 863\n",
      "Train Loss: 6841.327538491233\n",
      "Epoch 864\n",
      "Train Loss: 6841.327538461299\n",
      "Epoch 865\n",
      "Train Loss: 6841.3275384319795\n",
      "Epoch 866\n",
      "Train Loss: 6841.327538403265\n",
      "Epoch 867\n",
      "Train Loss: 6841.32753837514\n",
      "Epoch 868\n",
      "Train Loss: 6841.327538347594\n",
      "Epoch 869\n",
      "Train Loss: 6841.327538320614\n",
      "Epoch 870\n",
      "Train Loss: 6841.327538294189\n",
      "Epoch 871\n",
      "Train Loss: 6841.327538268309\n",
      "Epoch 872\n",
      "Train Loss: 6841.32753824296\n",
      "Epoch 873\n",
      "Train Loss: 6841.327538218134\n",
      "Epoch 874\n",
      "Train Loss: 6841.327538193818\n",
      "Epoch 875\n",
      "Train Loss: 6841.3275381700005\n",
      "Epoch 876\n",
      "Train Loss: 6841.327538146676\n",
      "Epoch 877\n",
      "Train Loss: 6841.327538123829\n",
      "Epoch 878\n",
      "Train Loss: 6841.327538101452\n",
      "Epoch 879\n",
      "Train Loss: 6841.327538079536\n",
      "Epoch 880\n",
      "Train Loss: 6841.327538058072\n",
      "Epoch 881\n",
      "Train Loss: 6841.327538037048\n",
      "Epoch 882\n",
      "Train Loss: 6841.327538016457\n",
      "Epoch 883\n",
      "Train Loss: 6841.327537996289\n",
      "Epoch 884\n",
      "Train Loss: 6841.3275379765355\n",
      "Epoch 885\n",
      "Train Loss: 6841.3275379571905\n",
      "Epoch 886\n",
      "Train Loss: 6841.327537938243\n",
      "Epoch 887\n",
      "Train Loss: 6841.327537919682\n",
      "Epoch 888\n",
      "Train Loss: 6841.327537901506\n",
      "Epoch 889\n",
      "Train Loss: 6841.327537883704\n",
      "Epoch 890\n",
      "Train Loss: 6841.3275378662665\n",
      "Epoch 891\n",
      "Train Loss: 6841.327537849189\n",
      "Epoch 892\n",
      "Train Loss: 6841.3275378324615\n",
      "Epoch 893\n",
      "Train Loss: 6841.32753781608\n",
      "Epoch 894\n",
      "Train Loss: 6841.327537800034\n",
      "Epoch 895\n",
      "Train Loss: 6841.327537784318\n",
      "Epoch 896\n",
      "Train Loss: 6841.327537768926\n",
      "Epoch 897\n",
      "Train Loss: 6841.32753775385\n",
      "Epoch 898\n",
      "Train Loss: 6841.3275377390855\n",
      "Epoch 899\n",
      "Train Loss: 6841.327537724625\n",
      "Epoch 900\n",
      "Train Loss: 6841.32753771046\n",
      "Epoch 901\n",
      "Train Loss: 6841.327537696587\n",
      "Epoch 902\n",
      "Train Loss: 6841.327537682999\n",
      "Epoch 903\n",
      "Train Loss: 6841.32753766969\n",
      "Epoch 904\n",
      "Train Loss: 6841.327537656657\n",
      "Epoch 905\n",
      "Train Loss: 6841.327537643891\n",
      "Epoch 906\n",
      "Train Loss: 6841.327537631387\n",
      "Epoch 907\n",
      "Train Loss: 6841.327537619141\n",
      "Epoch 908\n",
      "Train Loss: 6841.327537607146\n",
      "Epoch 909\n",
      "Train Loss: 6841.327537595398\n",
      "Epoch 910\n",
      "Train Loss: 6841.327537583892\n",
      "Epoch 911\n",
      "Train Loss: 6841.327537572624\n",
      "Epoch 912\n",
      "Train Loss: 6841.327537561588\n",
      "Epoch 913\n",
      "Train Loss: 6841.327537550776\n",
      "Epoch 914\n",
      "Train Loss: 6841.327537540188\n",
      "Epoch 915\n",
      "Train Loss: 6841.327537529818\n",
      "Epoch 916\n",
      "Train Loss: 6841.32753751966\n",
      "Epoch 917\n",
      "Train Loss: 6841.327537509713\n",
      "Epoch 918\n",
      "Train Loss: 6841.3275374999685\n",
      "Epoch 919\n",
      "Train Loss: 6841.327537490425\n",
      "Epoch 920\n",
      "Train Loss: 6841.327537481079\n",
      "Epoch 921\n",
      "Train Loss: 6841.327537471926\n",
      "Epoch 922\n",
      "Train Loss: 6841.327537462959\n",
      "Epoch 923\n",
      "Train Loss: 6841.327537454176\n",
      "Epoch 924\n",
      "Train Loss: 6841.327537445576\n",
      "Epoch 925\n",
      "Train Loss: 6841.327537437152\n",
      "Epoch 926\n",
      "Train Loss: 6841.327537428901\n",
      "Epoch 927\n",
      "Train Loss: 6841.327537420819\n",
      "Epoch 928\n",
      "Train Loss: 6841.327537412904\n",
      "Epoch 929\n",
      "Train Loss: 6841.327537405154\n",
      "Epoch 930\n",
      "Train Loss: 6841.327537397561\n",
      "Epoch 931\n",
      "Train Loss: 6841.327537390125\n",
      "Epoch 932\n",
      "Train Loss: 6841.327537382841\n",
      "Epoch 933\n",
      "Train Loss: 6841.327537375707\n",
      "Epoch 934\n",
      "Train Loss: 6841.327537368719\n",
      "Epoch 935\n",
      "Train Loss: 6841.327537361877\n",
      "Epoch 936\n",
      "Train Loss: 6841.327537355175\n",
      "Epoch 937\n",
      "Train Loss: 6841.32753734861\n",
      "Epoch 938\n",
      "Train Loss: 6841.327537342181\n",
      "Epoch 939\n",
      "Train Loss: 6841.327537335884\n",
      "Epoch 940\n",
      "Train Loss: 6841.327537329716\n",
      "Epoch 941\n",
      "Train Loss: 6841.327537323675\n",
      "Epoch 942\n",
      "Train Loss: 6841.3275373177585\n",
      "Epoch 943\n",
      "Train Loss: 6841.327537311963\n",
      "Epoch 944\n",
      "Train Loss: 6841.327537306287\n",
      "Epoch 945\n",
      "Train Loss: 6841.327537300729\n",
      "Epoch 946\n",
      "Train Loss: 6841.327537295284\n",
      "Epoch 947\n",
      "Train Loss: 6841.3275372899525\n",
      "Epoch 948\n",
      "Train Loss: 6841.327537284729\n",
      "Epoch 949\n",
      "Train Loss: 6841.327537279613\n",
      "Epoch 950\n",
      "Train Loss: 6841.327537274602\n",
      "Epoch 951\n",
      "Train Loss: 6841.327537269696\n",
      "Epoch 952\n",
      "Train Loss: 6841.3275372648895\n",
      "Epoch 953\n",
      "Train Loss: 6841.327537260182\n",
      "Epoch 954\n",
      "Train Loss: 6841.327537255572\n",
      "Epoch 955\n",
      "Train Loss: 6841.327537251055\n",
      "Epoch 956\n",
      "Train Loss: 6841.327537246633\n",
      "Epoch 957\n",
      "Train Loss: 6841.327537242301\n",
      "Epoch 958\n",
      "Train Loss: 6841.327537238058\n",
      "Epoch 959\n",
      "Train Loss: 6841.327537233902\n",
      "Epoch 960\n",
      "Train Loss: 6841.327537229834\n",
      "Epoch 961\n",
      "Train Loss: 6841.327537225848\n",
      "Epoch 962\n",
      "Train Loss: 6841.327537221942\n",
      "Epoch 963\n",
      "Train Loss: 6841.327537218119\n",
      "Epoch 964\n",
      "Train Loss: 6841.327537214373\n",
      "Epoch 965\n",
      "Train Loss: 6841.3275372107055\n",
      "Epoch 966\n",
      "Train Loss: 6841.327537207113\n",
      "Epoch 967\n",
      "Train Loss: 6841.327537203593\n",
      "Epoch 968\n",
      "Train Loss: 6841.327537200147\n",
      "Epoch 969\n",
      "Train Loss: 6841.32753719677\n",
      "Epoch 970\n",
      "Train Loss: 6841.327537193465\n",
      "Epoch 971\n",
      "Train Loss: 6841.327537190226\n",
      "Epoch 972\n",
      "Train Loss: 6841.327537187055\n",
      "Epoch 973\n",
      "Train Loss: 6841.327537183948\n",
      "Epoch 974\n",
      "Train Loss: 6841.327537180906\n",
      "Epoch 975\n",
      "Train Loss: 6841.327537177926\n",
      "Epoch 976\n",
      "Train Loss: 6841.327537175007\n",
      "Epoch 977\n",
      "Train Loss: 6841.327537172149\n",
      "Epoch 978\n",
      "Train Loss: 6841.327537169349\n",
      "Epoch 979\n",
      "Train Loss: 6841.327537166607\n",
      "Epoch 980\n",
      "Train Loss: 6841.327537163921\n",
      "Epoch 981\n",
      "Train Loss: 6841.32753716129\n",
      "Epoch 982\n",
      "Train Loss: 6841.327537158715\n",
      "Epoch 983\n",
      "Train Loss: 6841.32753715619\n",
      "Epoch 984\n",
      "Train Loss: 6841.327537153718\n",
      "Epoch 985\n",
      "Train Loss: 6841.327537151299\n",
      "Epoch 986\n",
      "Train Loss: 6841.327537148928\n",
      "Epoch 987\n",
      "Train Loss: 6841.327537146605\n",
      "Epoch 988\n",
      "Train Loss: 6841.327537144331\n",
      "Epoch 989\n",
      "Train Loss: 6841.327537142103\n",
      "Epoch 990\n",
      "Train Loss: 6841.327537139921\n",
      "Epoch 991\n",
      "Train Loss: 6841.327537137785\n",
      "Epoch 992\n",
      "Train Loss: 6841.327537135692\n",
      "Epoch 993\n",
      "Train Loss: 6841.327537133641\n",
      "Epoch 994\n",
      "Train Loss: 6841.327537131635\n",
      "Epoch 995\n",
      "Train Loss: 6841.327537129667\n",
      "Epoch 996\n",
      "Train Loss: 6841.327537127741\n",
      "Epoch 997\n",
      "Train Loss: 6841.327537125855\n",
      "Epoch 998\n",
      "Train Loss: 6841.327537124008\n",
      "Epoch 999\n",
      "Train Loss: 6841.327537122198\n",
      "Epoch 1000\n",
      "Train Loss: 6841.327537120426\n",
      "Epoch 1001\n",
      "Train Loss: 6841.32753711869\n",
      "Epoch 1002\n",
      "Train Loss: 6841.32753711699\n",
      "Epoch 1003\n",
      "Train Loss: 6841.327537115324\n",
      "Epoch 1004\n",
      "Train Loss: 6841.327537113693\n",
      "Epoch 1005\n",
      "Train Loss: 6841.327537112096\n",
      "Epoch 1006\n",
      "Train Loss: 6841.327537110531\n",
      "Epoch 1007\n",
      "Train Loss: 6841.327537108998\n",
      "Epoch 1008\n",
      "Train Loss: 6841.327537107498\n",
      "Epoch 1009\n",
      "Train Loss: 6841.327537106028\n",
      "Epoch 1010\n",
      "Train Loss: 6841.327537104588\n",
      "Epoch 1011\n",
      "Train Loss: 6841.327537103179\n",
      "Epoch 1012\n",
      "Train Loss: 6841.327537101797\n",
      "Epoch 1013\n",
      "Train Loss: 6841.327537100445\n",
      "Epoch 1014\n",
      "Train Loss: 6841.327537099119\n",
      "Epoch 1015\n",
      "Train Loss: 6841.327537097821\n",
      "Epoch 1016\n",
      "Train Loss: 6841.32753709655\n",
      "Epoch 1017\n",
      "Train Loss: 6841.327537095306\n",
      "Epoch 1018\n",
      "Train Loss: 6841.327537094086\n",
      "Epoch 1019\n",
      "Train Loss: 6841.327537092892\n",
      "Epoch 1020\n",
      "Train Loss: 6841.327537091723\n",
      "Epoch 1021\n",
      "Train Loss: 6841.327537090577\n",
      "Epoch 1022\n",
      "Train Loss: 6841.327537089455\n",
      "Epoch 1023\n",
      "Train Loss: 6841.327537088357\n",
      "Epoch 1024\n",
      "Train Loss: 6841.327537087281\n",
      "Epoch 1025\n",
      "Train Loss: 6841.327537086226\n",
      "Epoch 1026\n",
      "Train Loss: 6841.327537085193\n",
      "Epoch 1027\n",
      "Train Loss: 6841.327537084182\n",
      "Epoch 1028\n",
      "Train Loss: 6841.327537083192\n",
      "Epoch 1029\n",
      "Train Loss: 6841.327537082222\n",
      "Epoch 1030\n",
      "Train Loss: 6841.32753708127\n",
      "Epoch 1031\n",
      "Train Loss: 6841.327537080341\n",
      "Epoch 1032\n",
      "Train Loss: 6841.32753707943\n",
      "Epoch 1033\n",
      "Train Loss: 6841.327537078537\n",
      "Epoch 1034\n",
      "Train Loss: 6841.327537077662\n",
      "Epoch 1035\n",
      "Train Loss: 6841.327537076806\n",
      "Epoch 1036\n",
      "Train Loss: 6841.3275370759675\n",
      "Epoch 1037\n",
      "Train Loss: 6841.327537075146\n",
      "Epoch 1038\n",
      "Train Loss: 6841.327537074342\n",
      "Epoch 1039\n",
      "Train Loss: 6841.327537073554\n",
      "Epoch 1040\n",
      "Train Loss: 6841.327537072782\n",
      "Epoch 1041\n",
      "Train Loss: 6841.327537072025\n",
      "Epoch 1042\n",
      "Train Loss: 6841.327537071285\n",
      "Epoch 1043\n",
      "Train Loss: 6841.3275370705605\n",
      "Epoch 1044\n",
      "Train Loss: 6841.32753706985\n",
      "Epoch 1045\n",
      "Train Loss: 6841.3275370691545\n",
      "Epoch 1046\n",
      "Train Loss: 6841.327537068472\n",
      "Epoch 1047\n",
      "Train Loss: 6841.327537067806\n",
      "Epoch 1048\n",
      "Train Loss: 6841.327537067151\n",
      "Epoch 1049\n",
      "Train Loss: 6841.3275370665115\n",
      "Epoch 1050\n",
      "Train Loss: 6841.327537065885\n",
      "Epoch 1051\n",
      "Train Loss: 6841.327537065269\n",
      "Epoch 1052\n",
      "Train Loss: 6841.327537064668\n",
      "Epoch 1053\n",
      "Train Loss: 6841.3275370640795\n",
      "Epoch 1054\n",
      "Train Loss: 6841.327537063502\n",
      "Epoch 1055\n",
      "Train Loss: 6841.327537062938\n",
      "Epoch 1056\n",
      "Train Loss: 6841.327537062385\n",
      "Epoch 1057\n",
      "Train Loss: 6841.327537061842\n",
      "Epoch 1058\n",
      "Train Loss: 6841.327537061311\n",
      "Epoch 1059\n",
      "Train Loss: 6841.327537060791\n",
      "Epoch 1060\n",
      "Train Loss: 6841.3275370602805\n",
      "Epoch 1061\n",
      "Train Loss: 6841.327537059782\n",
      "Epoch 1062\n",
      "Train Loss: 6841.327537059293\n",
      "Epoch 1063\n",
      "Train Loss: 6841.327537058814\n",
      "Epoch 1064\n",
      "Train Loss: 6841.327537058347\n",
      "Epoch 1065\n",
      "Train Loss: 6841.327537057887\n",
      "Epoch 1066\n",
      "Train Loss: 6841.327537057438\n",
      "Epoch 1067\n",
      "Train Loss: 6841.327537056997\n",
      "Epoch 1068\n",
      "Train Loss: 6841.327537056566\n",
      "Epoch 1069\n",
      "Train Loss: 6841.327537056144\n",
      "Epoch 1070\n",
      "Train Loss: 6841.32753705573\n",
      "Epoch 1071\n",
      "Train Loss: 6841.327537055325\n",
      "Epoch 1072\n",
      "Train Loss: 6841.327537054927\n",
      "Epoch 1073\n",
      "Train Loss: 6841.327537054539\n",
      "Epoch 1074\n",
      "Train Loss: 6841.327537054158\n",
      "Epoch 1075\n",
      "Train Loss: 6841.327537053785\n",
      "Epoch 1076\n",
      "Train Loss: 6841.327537053419\n",
      "Epoch 1077\n",
      "Train Loss: 6841.327537053062\n",
      "Epoch 1078\n",
      "Train Loss: 6841.327537052712\n",
      "Epoch 1079\n",
      "Train Loss: 6841.327537052368\n",
      "Epoch 1080\n",
      "Train Loss: 6841.327537052031\n",
      "Epoch 1081\n",
      "Train Loss: 6841.327537051704\n",
      "Epoch 1082\n",
      "Train Loss: 6841.32753705138\n",
      "Epoch 1083\n",
      "Train Loss: 6841.327537051065\n",
      "Epoch 1084\n",
      "Train Loss: 6841.327537050755\n",
      "Epoch 1085\n",
      "Train Loss: 6841.3275370504525\n",
      "Epoch 1086\n",
      "Train Loss: 6841.327537050155\n",
      "Epoch 1087\n",
      "Train Loss: 6841.327537049864\n",
      "Epoch 1088\n",
      "Train Loss: 6841.327537049581\n",
      "Epoch 1089\n",
      "Train Loss: 6841.327537049301\n",
      "Epoch 1090\n",
      "Train Loss: 6841.327537049029\n",
      "Epoch 1091\n",
      "Train Loss: 6841.327537048761\n",
      "Epoch 1092\n",
      "Train Loss: 6841.3275370485\n",
      "Epoch 1093\n",
      "Train Loss: 6841.327537048243\n",
      "Epoch 1094\n",
      "Train Loss: 6841.327537047992\n",
      "Epoch 1095\n",
      "Train Loss: 6841.327537047745\n",
      "Epoch 1096\n",
      "Train Loss: 6841.327537047505\n",
      "Epoch 1097\n",
      "Train Loss: 6841.327537047268\n",
      "Epoch 1098\n",
      "Train Loss: 6841.3275370470365\n",
      "Epoch 1099\n",
      "Train Loss: 6841.32753704681\n",
      "Epoch 1100\n",
      "Train Loss: 6841.327537046589\n",
      "Epoch 1101\n",
      "Train Loss: 6841.3275370463725\n",
      "Epoch 1102\n",
      "Train Loss: 6841.327537046157\n",
      "Epoch 1103\n",
      "Train Loss: 6841.327537045949\n",
      "Epoch 1104\n",
      "Train Loss: 6841.327537045746\n",
      "Epoch 1105\n",
      "Train Loss: 6841.327537045546\n",
      "Epoch 1106\n",
      "Train Loss: 6841.32753704535\n",
      "Epoch 1107\n",
      "Train Loss: 6841.327537045158\n",
      "Epoch 1108\n",
      "Train Loss: 6841.32753704497\n",
      "Epoch 1109\n",
      "Train Loss: 6841.327537044786\n",
      "Epoch 1110\n",
      "Train Loss: 6841.327537044607\n",
      "Epoch 1111\n",
      "Train Loss: 6841.32753704443\n",
      "Epoch 1112\n",
      "Train Loss: 6841.327537044256\n",
      "Epoch 1113\n",
      "Train Loss: 6841.327537044088\n",
      "Epoch 1114\n",
      "Train Loss: 6841.327537043921\n",
      "Epoch 1115\n",
      "Train Loss: 6841.327537043759\n",
      "Epoch 1116\n",
      "Train Loss: 6841.3275370436\n",
      "Epoch 1117\n",
      "Train Loss: 6841.327537043445\n",
      "Epoch 1118\n",
      "Train Loss: 6841.327537043292\n",
      "Epoch 1119\n",
      "Train Loss: 6841.327537043143\n",
      "Epoch 1120\n",
      "Train Loss: 6841.327537042996\n",
      "Epoch 1121\n",
      "Train Loss: 6841.327537042853\n",
      "Epoch 1122\n",
      "Train Loss: 6841.327537042712\n",
      "Epoch 1123\n",
      "Train Loss: 6841.3275370425745\n",
      "Epoch 1124\n",
      "Train Loss: 6841.32753704244\n",
      "Epoch 1125\n",
      "Train Loss: 6841.327537042308\n",
      "Epoch 1126\n",
      "Train Loss: 6841.32753704218\n",
      "Epoch 1127\n",
      "Train Loss: 6841.327537042052\n",
      "Epoch 1128\n",
      "Train Loss: 6841.327537041928\n",
      "Epoch 1129\n",
      "Train Loss: 6841.327537041807\n",
      "Epoch 1130\n",
      "Train Loss: 6841.327537041688\n",
      "Epoch 1131\n",
      "Train Loss: 6841.327537041571\n",
      "Epoch 1132\n",
      "Train Loss: 6841.327537041458\n",
      "Epoch 1133\n",
      "Train Loss: 6841.327537041345\n",
      "Epoch 1134\n",
      "Train Loss: 6841.327537041237\n",
      "Epoch 1135\n",
      "Train Loss: 6841.327537041129\n",
      "Epoch 1136\n",
      "Train Loss: 6841.327537041023\n",
      "Epoch 1137\n",
      "Train Loss: 6841.327537040921\n",
      "Epoch 1138\n",
      "Train Loss: 6841.32753704082\n",
      "Epoch 1139\n",
      "Train Loss: 6841.327537040722\n",
      "Epoch 1140\n",
      "Train Loss: 6841.327537040625\n",
      "Epoch 1141\n",
      "Train Loss: 6841.327537040531\n",
      "Epoch 1142\n",
      "Train Loss: 6841.327537040438\n",
      "Epoch 1143\n",
      "Train Loss: 6841.327537040347\n",
      "Epoch 1144\n",
      "Train Loss: 6841.327537040259\n",
      "Epoch 1145\n",
      "Train Loss: 6841.327537040171\n",
      "Epoch 1146\n",
      "Train Loss: 6841.327537040085\n",
      "Epoch 1147\n",
      "Train Loss: 6841.3275370400015\n",
      "Epoch 1148\n",
      "Train Loss: 6841.32753703992\n",
      "Epoch 1149\n",
      "Train Loss: 6841.3275370398405\n",
      "Epoch 1150\n",
      "Train Loss: 6841.327537039762\n",
      "Epoch 1151\n",
      "Train Loss: 6841.327537039685\n",
      "Epoch 1152\n",
      "Train Loss: 6841.3275370396095\n",
      "Epoch 1153\n",
      "Train Loss: 6841.327537039536\n",
      "Epoch 1154\n",
      "Train Loss: 6841.327537039464\n",
      "Epoch 1155\n",
      "Train Loss: 6841.327537039393\n",
      "Epoch 1156\n",
      "Train Loss: 6841.327537039323\n",
      "Epoch 1157\n",
      "Train Loss: 6841.327537039255\n",
      "Epoch 1158\n",
      "Train Loss: 6841.327537039189\n",
      "Epoch 1159\n",
      "Train Loss: 6841.327537039125\n",
      "Epoch 1160\n",
      "Train Loss: 6841.32753703906\n",
      "Epoch 1161\n",
      "Train Loss: 6841.327537038999\n",
      "Epoch 1162\n",
      "Train Loss: 6841.327537038937\n",
      "Epoch 1163\n",
      "Train Loss: 6841.3275370388765\n",
      "Epoch 1164\n",
      "Train Loss: 6841.327537038818\n",
      "Epoch 1165\n",
      "Train Loss: 6841.327537038761\n",
      "Epoch 1166\n",
      "Train Loss: 6841.3275370387055\n",
      "Epoch 1167\n",
      "Train Loss: 6841.327537038649\n",
      "Epoch 1168\n",
      "Train Loss: 6841.3275370385945\n",
      "Epoch 1169\n",
      "Train Loss: 6841.327537038542\n",
      "Epoch 1170\n",
      "Train Loss: 6841.327537038491\n",
      "Epoch 1171\n",
      "Train Loss: 6841.32753703844\n"
     ]
    }
   ],
   "source": [
    "opt: str = \"gd\"\n",
    "lr: float = 0.5\n",
    "gd_w: np.ndarray; gd_history: pd.DataFrame\n",
    "gd_w, gd_history = train(train_K, train_Y, lamb, lr, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 6931.471805599453\n",
      "Epoch 2\n",
      "Train Loss: 6927.227267318698\n",
      "Epoch 3\n",
      "Train Loss: 6923.365962247177\n",
      "Epoch 4\n",
      "Train Loss: 6919.859257948226\n",
      "Epoch 5\n",
      "Train Loss: 6916.575555691646\n",
      "Epoch 6\n",
      "Train Loss: 6913.469467906016\n",
      "Epoch 7\n",
      "Train Loss: 6910.504554327614\n",
      "Epoch 8\n",
      "Train Loss: 6907.687334863922\n",
      "Epoch 9\n",
      "Train Loss: 6904.986488158478\n",
      "Epoch 10\n",
      "Train Loss: 6902.40427824877\n",
      "Epoch 11\n",
      "Train Loss: 6899.919776167033\n",
      "Epoch 12\n",
      "Train Loss: 6897.543378320982\n",
      "Epoch 13\n",
      "Train Loss: 6895.261582162569\n",
      "Epoch 14\n",
      "Train Loss: 6893.074284775699\n",
      "Epoch 15\n",
      "Train Loss: 6890.974397147163\n",
      "Epoch 16\n",
      "Train Loss: 6888.960760145568\n",
      "Epoch 17\n",
      "Train Loss: 6887.029030424567\n",
      "Epoch 18\n",
      "Train Loss: 6885.176492774661\n",
      "Epoch 19\n",
      "Train Loss: 6883.3986270994665\n",
      "Epoch 20\n",
      "Train Loss: 6881.692211351112\n",
      "Epoch 21\n",
      "Train Loss: 6880.0559401365645\n",
      "Epoch 22\n",
      "Train Loss: 6878.484976627482\n",
      "Epoch 23\n",
      "Train Loss: 6876.978599095684\n",
      "Epoch 24\n",
      "Train Loss: 6875.532682172061\n",
      "Epoch 25\n",
      "Train Loss: 6874.145915190982\n",
      "Epoch 26\n",
      "Train Loss: 6872.815116166146\n",
      "Epoch 27\n",
      "Train Loss: 6871.53800625047\n",
      "Epoch 28\n",
      "Train Loss: 6870.312937098883\n",
      "Epoch 29\n",
      "Train Loss: 6869.137670638222\n",
      "Epoch 30\n",
      "Train Loss: 6868.0098902878935\n",
      "Epoch 31\n",
      "Train Loss: 6866.92801034502\n",
      "Epoch 32\n",
      "Train Loss: 6865.890136341055\n",
      "Epoch 33\n",
      "Train Loss: 6864.894539605093\n",
      "Epoch 34\n",
      "Train Loss: 6863.938934991836\n",
      "Epoch 35\n",
      "Train Loss: 6863.022762662657\n",
      "Epoch 36\n",
      "Train Loss: 6862.142295171763\n",
      "Epoch 37\n",
      "Train Loss: 6861.298383763478\n",
      "Epoch 38\n",
      "Train Loss: 6860.488665599728\n",
      "Epoch 39\n",
      "Train Loss: 6859.711644739672\n",
      "Epoch 40\n",
      "Train Loss: 6858.966209734478\n",
      "Epoch 41\n",
      "Train Loss: 6858.251443366928\n",
      "Epoch 42\n",
      "Train Loss: 6857.56548297411\n",
      "Epoch 43\n",
      "Train Loss: 6856.908718443129\n",
      "Epoch 44\n",
      "Train Loss: 6856.275401409998\n",
      "Epoch 45\n",
      "Train Loss: 6855.671086607433\n",
      "Epoch 46\n",
      "Train Loss: 6855.087437939959\n",
      "Epoch 47\n",
      "Train Loss: 6854.529331864885\n",
      "Epoch 48\n",
      "Train Loss: 6853.9942866538695\n",
      "Epoch 49\n",
      "Train Loss: 6853.480966378114\n",
      "Epoch 50\n",
      "Train Loss: 6852.987661662829\n",
      "Epoch 51\n",
      "Train Loss: 6852.51473270739\n",
      "Epoch 52\n",
      "Train Loss: 6852.061117312347\n",
      "Epoch 53\n",
      "Train Loss: 6851.6257293588405\n",
      "Epoch 54\n",
      "Train Loss: 6851.2081892036995\n",
      "Epoch 55\n",
      "Train Loss: 6850.807505016497\n",
      "Epoch 56\n",
      "Train Loss: 6850.423694163714\n",
      "Epoch 57\n",
      "Train Loss: 6850.054608686485\n",
      "Epoch 58\n",
      "Train Loss: 6849.701501415247\n",
      "Epoch 59\n",
      "Train Loss: 6849.361966939921\n",
      "Epoch 60\n",
      "Train Loss: 6849.036576270306\n",
      "Epoch 61\n",
      "Train Loss: 6848.72410583043\n",
      "Epoch 62\n",
      "Train Loss: 6848.423274351342\n",
      "Epoch 63\n",
      "Train Loss: 6848.135302193006\n",
      "Epoch 64\n",
      "Train Loss: 6847.859320996171\n",
      "Epoch 65\n",
      "Train Loss: 6847.594599584367\n",
      "Epoch 66\n",
      "Train Loss: 6847.340969386113\n",
      "Epoch 67\n",
      "Train Loss: 6847.098352908498\n",
      "Epoch 68\n",
      "Train Loss: 6846.863780848652\n",
      "Epoch 69\n",
      "Train Loss: 6846.639976711299\n",
      "Epoch 70\n",
      "Train Loss: 6846.422754356236\n",
      "Epoch 71\n",
      "Train Loss: 6846.216318717967\n",
      "Epoch 72\n",
      "Train Loss: 6846.020843244194\n",
      "Epoch 73\n",
      "Train Loss: 6845.8314568921905\n",
      "Epoch 74\n",
      "Train Loss: 6845.646784339993\n",
      "Epoch 75\n",
      "Train Loss: 6845.47066848206\n",
      "Epoch 76\n",
      "Train Loss: 6845.303775324748\n",
      "Epoch 77\n",
      "Train Loss: 6845.141268830849\n",
      "Epoch 78\n",
      "Train Loss: 6844.9873597805035\n",
      "Epoch 79\n",
      "Train Loss: 6844.83857529332\n",
      "Epoch 80\n",
      "Train Loss: 6844.696506207894\n",
      "Epoch 81\n",
      "Train Loss: 6844.559191375151\n",
      "Epoch 82\n",
      "Train Loss: 6844.428379137532\n",
      "Epoch 83\n",
      "Train Loss: 6844.302684862616\n",
      "Epoch 84\n",
      "Train Loss: 6844.1821277132285\n",
      "Epoch 85\n",
      "Train Loss: 6844.066333846674\n",
      "Epoch 86\n",
      "Train Loss: 6843.955021549664\n",
      "Epoch 87\n",
      "Train Loss: 6843.848517251162\n",
      "Epoch 88\n",
      "Train Loss: 6843.746435060038\n",
      "Epoch 89\n",
      "Train Loss: 6843.648939558332\n",
      "Epoch 90\n",
      "Train Loss: 6843.5545976187805\n",
      "Epoch 91\n",
      "Train Loss: 6843.463921839124\n",
      "Epoch 92\n",
      "Train Loss: 6843.377866458505\n",
      "Epoch 93\n",
      "Train Loss: 6843.294512451035\n",
      "Epoch 94\n",
      "Train Loss: 6843.214373553987\n",
      "Epoch 95\n",
      "Train Loss: 6843.137912987022\n",
      "Epoch 96\n",
      "Train Loss: 6843.0645041374055\n",
      "Epoch 97\n",
      "Train Loss: 6842.9950781887455\n",
      "Epoch 98\n",
      "Train Loss: 6842.9264439770795\n",
      "Epoch 99\n",
      "Train Loss: 6842.861627108573\n",
      "Epoch 100\n",
      "Train Loss: 6842.799532767916\n",
      "Epoch 101\n",
      "Train Loss: 6842.73991442859\n",
      "Epoch 102\n",
      "Train Loss: 6842.68376081422\n",
      "Epoch 103\n",
      "Train Loss: 6842.628126614204\n",
      "Epoch 104\n",
      "Train Loss: 6842.575082694919\n",
      "Epoch 105\n",
      "Train Loss: 6842.5244277531065\n",
      "Epoch 106\n",
      "Train Loss: 6842.475891778702\n",
      "Epoch 107\n",
      "Train Loss: 6842.429494253813\n",
      "Epoch 108\n",
      "Train Loss: 6842.384658511943\n",
      "Epoch 109\n",
      "Train Loss: 6842.342994306938\n",
      "Epoch 110\n",
      "Train Loss: 6842.300492620167\n",
      "Epoch 111\n",
      "Train Loss: 6842.261031477862\n",
      "Epoch 112\n",
      "Train Loss: 6842.223189764398\n",
      "Epoch 113\n",
      "Train Loss: 6842.186962636025\n",
      "Epoch 114\n",
      "Train Loss: 6842.152374660932\n",
      "Epoch 115\n",
      "Train Loss: 6842.119514563422\n",
      "Epoch 116\n",
      "Train Loss: 6842.088402897528\n",
      "Epoch 117\n",
      "Train Loss: 6842.057527033416\n",
      "Epoch 118\n",
      "Train Loss: 6842.027452557503\n",
      "Epoch 119\n",
      "Train Loss: 6841.998571447495\n",
      "Epoch 120\n",
      "Train Loss: 6841.970908807798\n",
      "Epoch 121\n",
      "Train Loss: 6841.945194830236\n",
      "Epoch 122\n",
      "Train Loss: 6841.919627020244\n",
      "Epoch 123\n",
      "Train Loss: 6841.896830919024\n",
      "Epoch 124\n",
      "Train Loss: 6841.87335396932\n",
      "Epoch 125\n",
      "Train Loss: 6841.850541292317\n",
      "Epoch 126\n",
      "Train Loss: 6841.829477461445\n",
      "Epoch 127\n",
      "Train Loss: 6841.809001235693\n",
      "Epoch 128\n",
      "Train Loss: 6841.789614111992\n",
      "Epoch 129\n",
      "Train Loss: 6841.770902927247\n",
      "Epoch 130\n",
      "Train Loss: 6841.752784904419\n",
      "Epoch 131\n",
      "Train Loss: 6841.735556552887\n",
      "Epoch 132\n",
      "Train Loss: 6841.7189778068405\n",
      "Epoch 133\n",
      "Train Loss: 6841.703113867528\n",
      "Epoch 134\n",
      "Train Loss: 6841.688010088505\n",
      "Epoch 135\n",
      "Train Loss: 6841.6732805345855\n",
      "Epoch 136\n",
      "Train Loss: 6841.659266355972\n",
      "Epoch 137\n",
      "Train Loss: 6841.645976648642\n",
      "Epoch 138\n",
      "Train Loss: 6841.633103551111\n",
      "Epoch 139\n",
      "Train Loss: 6841.620690783557\n",
      "Epoch 140\n",
      "Train Loss: 6841.60884240115\n",
      "Epoch 141\n",
      "Train Loss: 6841.598277069018\n",
      "Epoch 142\n",
      "Train Loss: 6841.586574082779\n",
      "Epoch 143\n",
      "Train Loss: 6841.5769143255075\n",
      "Epoch 144\n",
      "Train Loss: 6841.5668340711345\n",
      "Epoch 145\n",
      "Train Loss: 6841.556284917191\n",
      "Epoch 146\n",
      "Train Loss: 6841.546896162975\n",
      "Epoch 147\n",
      "Train Loss: 6841.538007368195\n",
      "Epoch 148\n",
      "Train Loss: 6841.529380753677\n",
      "Epoch 149\n",
      "Train Loss: 6841.521176417727\n",
      "Epoch 150\n",
      "Train Loss: 6841.513398256744\n",
      "Epoch 151\n",
      "Train Loss: 6841.506049227828\n",
      "Epoch 152\n",
      "Train Loss: 6841.4989290340745\n",
      "Epoch 153\n",
      "Train Loss: 6841.491697983081\n",
      "Epoch 154\n",
      "Train Loss: 6841.48541817249\n",
      "Epoch 155\n",
      "Train Loss: 6841.478715995458\n",
      "Epoch 156\n",
      "Train Loss: 6841.472513252997\n",
      "Epoch 157\n",
      "Train Loss: 6841.466844263476\n",
      "Epoch 158\n",
      "Train Loss: 6841.46098619361\n",
      "Epoch 159\n",
      "Train Loss: 6841.4555890145275\n",
      "Epoch 160\n",
      "Train Loss: 6841.450403667254\n",
      "Epoch 161\n",
      "Train Loss: 6841.445481938708\n",
      "Epoch 162\n",
      "Train Loss: 6841.440829478359\n",
      "Epoch 163\n",
      "Train Loss: 6841.436081402824\n",
      "Epoch 164\n",
      "Train Loss: 6841.431897676444\n",
      "Epoch 165\n",
      "Train Loss: 6841.428030515435\n",
      "Epoch 166\n",
      "Train Loss: 6841.423407457085\n",
      "Epoch 167\n",
      "Train Loss: 6841.419620452059\n",
      "Epoch 168\n",
      "Train Loss: 6841.416504938953\n",
      "Epoch 169\n",
      "Train Loss: 6841.413155617001\n",
      "Epoch 170\n",
      "Train Loss: 6841.409606497102\n",
      "Epoch 171\n",
      "Train Loss: 6841.407558779053\n",
      "Epoch 172\n",
      "Train Loss: 6841.403144602202\n",
      "Epoch 173\n",
      "Train Loss: 6841.399619005781\n",
      "Epoch 174\n",
      "Train Loss: 6841.396645714698\n",
      "Epoch 175\n",
      "Train Loss: 6841.393601794785\n",
      "Epoch 176\n",
      "Train Loss: 6841.390955947916\n",
      "Epoch 177\n",
      "Train Loss: 6841.3885012805\n",
      "Epoch 178\n",
      "Train Loss: 6841.386435729715\n",
      "Epoch 179\n",
      "Train Loss: 6841.38375552612\n",
      "Epoch 180\n",
      "Train Loss: 6841.382993316917\n"
     ]
    }
   ],
   "source": [
    "opt: str = \"sgd\"\n",
    "lr: float = 1\n",
    "p: int = 100\n",
    "sgd_100_w: np.ndarray; sgd_100_history: pd.DataFrame\n",
    "sgd_100_w, sgd_100_history = train(train_K, train_Y, lamb, lr, opt, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 6931.471805599453\n",
      "Epoch 2\n",
      "Train Loss: 6927.19792757019\n",
      "Epoch 3\n",
      "Train Loss: 6923.37300399484\n",
      "Epoch 4\n",
      "Train Loss: 6919.866486685332\n",
      "Epoch 5\n",
      "Train Loss: 6916.582239849215\n",
      "Epoch 6\n",
      "Train Loss: 6913.476181363428\n",
      "Epoch 7\n",
      "Train Loss: 6910.507028635557\n",
      "Epoch 8\n",
      "Train Loss: 6907.678288466176\n",
      "Epoch 9\n",
      "Train Loss: 6904.983155367272\n",
      "Epoch 10\n",
      "Train Loss: 6902.396967593785\n",
      "Epoch 11\n",
      "Train Loss: 6899.920791998916\n",
      "Epoch 12\n",
      "Train Loss: 6897.542754625151\n",
      "Epoch 13\n",
      "Train Loss: 6895.262489609098\n",
      "Epoch 14\n",
      "Train Loss: 6893.076928882649\n",
      "Epoch 15\n",
      "Train Loss: 6890.980110321318\n",
      "Epoch 16\n",
      "Train Loss: 6888.965186648916\n",
      "Epoch 17\n",
      "Train Loss: 6887.034324169301\n",
      "Epoch 18\n",
      "Train Loss: 6885.180151425725\n",
      "Epoch 19\n",
      "Train Loss: 6883.402020681152\n",
      "Epoch 20\n",
      "Train Loss: 6881.695971242311\n",
      "Epoch 21\n",
      "Train Loss: 6880.05918784102\n",
      "Epoch 22\n",
      "Train Loss: 6878.489201512184\n",
      "Epoch 23\n",
      "Train Loss: 6876.982298143215\n",
      "Epoch 24\n",
      "Train Loss: 6875.537140279836\n",
      "Epoch 25\n",
      "Train Loss: 6874.149556583999\n",
      "Epoch 26\n",
      "Train Loss: 6872.818912067516\n",
      "Epoch 27\n",
      "Train Loss: 6871.542261247772\n",
      "Epoch 28\n",
      "Train Loss: 6870.317055991537\n",
      "Epoch 29\n",
      "Train Loss: 6869.141742163829\n",
      "Epoch 30\n",
      "Train Loss: 6868.014102512774\n",
      "Epoch 31\n",
      "Train Loss: 6866.9322051370855\n",
      "Epoch 32\n",
      "Train Loss: 6865.894720312405\n",
      "Epoch 33\n",
      "Train Loss: 6864.898184249235\n",
      "Epoch 34\n",
      "Train Loss: 6863.942455469356\n",
      "Epoch 35\n",
      "Train Loss: 6863.025631177636\n",
      "Epoch 36\n",
      "Train Loss: 6862.146443655033\n",
      "Epoch 37\n",
      "Train Loss: 6861.302054341163\n",
      "Epoch 38\n",
      "Train Loss: 6860.492269292272\n",
      "Epoch 39\n",
      "Train Loss: 6859.715928592525\n",
      "Epoch 40\n",
      "Train Loss: 6858.970119674736\n",
      "Epoch 41\n",
      "Train Loss: 6858.254238150575\n",
      "Epoch 42\n",
      "Train Loss: 6857.56778427592\n",
      "Epoch 43\n",
      "Train Loss: 6856.9092690822845\n",
      "Epoch 44\n",
      "Train Loss: 6856.277251246876\n",
      "Epoch 45\n",
      "Train Loss: 6855.6712647028\n",
      "Epoch 46\n",
      "Train Loss: 6855.08993976351\n",
      "Epoch 47\n",
      "Train Loss: 6854.532212441716\n",
      "Epoch 48\n",
      "Train Loss: 6853.9976056027235\n",
      "Epoch 49\n",
      "Train Loss: 6853.483847535532\n",
      "Epoch 50\n",
      "Train Loss: 6852.990920463981\n",
      "Epoch 51\n",
      "Train Loss: 6852.518109146741\n",
      "Epoch 52\n",
      "Train Loss: 6852.064940514882\n",
      "Epoch 53\n",
      "Train Loss: 6851.629357479884\n",
      "Epoch 54\n",
      "Train Loss: 6851.2107676850865\n",
      "Epoch 55\n",
      "Train Loss: 6850.81130467744\n",
      "Epoch 56\n",
      "Train Loss: 6850.425306063372\n",
      "Epoch 57\n",
      "Train Loss: 6850.057855001106\n",
      "Epoch 58\n",
      "Train Loss: 6849.7053271476525\n",
      "Epoch 59\n",
      "Train Loss: 6849.36463302317\n",
      "Epoch 60\n",
      "Train Loss: 6849.041321064107\n",
      "Epoch 61\n",
      "Train Loss: 6848.72617174928\n",
      "Epoch 62\n",
      "Train Loss: 6848.426073060717\n",
      "Epoch 63\n",
      "Train Loss: 6848.1381170193545\n",
      "Epoch 64\n",
      "Train Loss: 6847.862366774941\n",
      "Epoch 65\n",
      "Train Loss: 6847.596921181742\n",
      "Epoch 66\n",
      "Train Loss: 6847.342453021511\n",
      "Epoch 67\n",
      "Train Loss: 6847.098185814396\n",
      "Epoch 68\n",
      "Train Loss: 6846.864160119433\n",
      "Epoch 69\n",
      "Train Loss: 6846.640460569926\n",
      "Epoch 70\n",
      "Train Loss: 6846.424974882714\n",
      "Epoch 71\n",
      "Train Loss: 6846.219616865764\n",
      "Epoch 72\n",
      "Train Loss: 6846.0203650507065\n",
      "Epoch 73\n",
      "Train Loss: 6845.829144680115\n",
      "Epoch 74\n",
      "Train Loss: 6845.647147808702\n",
      "Epoch 75\n",
      "Train Loss: 6845.471617227764\n",
      "Epoch 76\n",
      "Train Loss: 6845.30332447036\n",
      "Epoch 77\n",
      "Train Loss: 6845.142825513967\n",
      "Epoch 78\n",
      "Train Loss: 6844.988297120165\n",
      "Epoch 79\n",
      "Train Loss: 6844.83985756538\n",
      "Epoch 80\n",
      "Train Loss: 6844.697639061163\n",
      "Epoch 81\n",
      "Train Loss: 6844.560216232452\n",
      "Epoch 82\n",
      "Train Loss: 6844.429145135606\n",
      "Epoch 83\n",
      "Train Loss: 6844.303397145043\n",
      "Epoch 84\n",
      "Train Loss: 6844.182734356158\n",
      "Epoch 85\n",
      "Train Loss: 6844.067252563046\n",
      "Epoch 86\n",
      "Train Loss: 6843.955940634628\n",
      "Epoch 87\n",
      "Train Loss: 6843.8494969513295\n",
      "Epoch 88\n",
      "Train Loss: 6843.7472793205025\n",
      "Epoch 89\n",
      "Train Loss: 6843.649154142952\n",
      "Epoch 90\n",
      "Train Loss: 6843.55573600949\n",
      "Epoch 91\n",
      "Train Loss: 6843.466165742947\n",
      "Epoch 92\n",
      "Train Loss: 6843.378723766332\n",
      "Epoch 93\n",
      "Train Loss: 6843.295280227439\n",
      "Epoch 94\n",
      "Train Loss: 6843.21516735135\n",
      "Epoch 95\n",
      "Train Loss: 6843.1388058068005\n",
      "Epoch 96\n",
      "Train Loss: 6843.065277443106\n",
      "Epoch 97\n",
      "Train Loss: 6842.99479306842\n",
      "Epoch 98\n",
      "Train Loss: 6842.927384871607\n",
      "Epoch 99\n",
      "Train Loss: 6842.8624595089905\n",
      "Epoch 100\n",
      "Train Loss: 6842.80006284144\n",
      "Epoch 101\n",
      "Train Loss: 6842.740377663521\n",
      "Epoch 102\n",
      "Train Loss: 6842.683110357027\n",
      "Epoch 103\n",
      "Train Loss: 6842.628052562481\n",
      "Epoch 104\n",
      "Train Loss: 6842.575414409483\n",
      "Epoch 105\n",
      "Train Loss: 6842.52468605307\n",
      "Epoch 106\n",
      "Train Loss: 6842.476178814848\n",
      "Epoch 107\n",
      "Train Loss: 6842.429645518295\n",
      "Epoch 108\n",
      "Train Loss: 6842.384994932189\n",
      "Epoch 109\n",
      "Train Loss: 6842.342305677659\n",
      "Epoch 110\n",
      "Train Loss: 6842.300893059308\n",
      "Epoch 111\n",
      "Train Loss: 6842.261553833392\n",
      "Epoch 112\n",
      "Train Loss: 6842.223524783022\n",
      "Epoch 113\n",
      "Train Loss: 6842.187383275022\n",
      "Epoch 114\n",
      "Train Loss: 6842.152692500069\n",
      "Epoch 115\n",
      "Train Loss: 6842.119526207901\n",
      "Epoch 116\n",
      "Train Loss: 6842.087583949227\n",
      "Epoch 117\n",
      "Train Loss: 6842.056754222382\n",
      "Epoch 118\n",
      "Train Loss: 6842.02751108297\n",
      "Epoch 119\n",
      "Train Loss: 6841.998863965592\n",
      "Epoch 120\n",
      "Train Loss: 6841.971969790624\n",
      "Epoch 121\n",
      "Train Loss: 6841.946264058876\n",
      "Epoch 122\n",
      "Train Loss: 6841.920488917608\n",
      "Epoch 123\n",
      "Train Loss: 6841.896849150128\n",
      "Epoch 124\n",
      "Train Loss: 6841.874079415491\n",
      "Epoch 125\n",
      "Train Loss: 6841.852875682578\n",
      "Epoch 126\n",
      "Train Loss: 6841.830069055403\n",
      "Epoch 127\n",
      "Train Loss: 6841.81009908432\n",
      "Epoch 128\n",
      "Train Loss: 6841.79111744916\n",
      "Epoch 129\n",
      "Train Loss: 6841.7713771021245\n",
      "Epoch 130\n",
      "Train Loss: 6841.753217569248\n",
      "Epoch 131\n",
      "Train Loss: 6841.73571630624\n",
      "Epoch 132\n",
      "Train Loss: 6841.7192933512815\n",
      "Epoch 133\n",
      "Train Loss: 6841.703482736036\n",
      "Epoch 134\n",
      "Train Loss: 6841.688456121005\n",
      "Epoch 135\n",
      "Train Loss: 6841.673385479944\n",
      "Epoch 136\n",
      "Train Loss: 6841.660288907427\n",
      "Epoch 137\n",
      "Train Loss: 6841.647547085811\n",
      "Epoch 138\n",
      "Train Loss: 6841.63429058803\n",
      "Epoch 139\n",
      "Train Loss: 6841.621797519788\n",
      "Epoch 140\n",
      "Train Loss: 6841.608883541435\n",
      "Epoch 141\n",
      "Train Loss: 6841.59754538271\n",
      "Epoch 142\n",
      "Train Loss: 6841.587595311718\n",
      "Epoch 143\n",
      "Train Loss: 6841.576305995136\n",
      "Epoch 144\n",
      "Train Loss: 6841.565811716452\n",
      "Epoch 145\n",
      "Train Loss: 6841.556148707719\n",
      "Epoch 146\n",
      "Train Loss: 6841.546877395503\n",
      "Epoch 147\n",
      "Train Loss: 6841.5386051819805\n",
      "Epoch 148\n",
      "Train Loss: 6841.529605390967\n",
      "Epoch 149\n",
      "Train Loss: 6841.521278394554\n",
      "Epoch 150\n",
      "Train Loss: 6841.51339462144\n",
      "Epoch 151\n",
      "Train Loss: 6841.50589372331\n",
      "Epoch 152\n",
      "Train Loss: 6841.49863217595\n",
      "Epoch 153\n",
      "Train Loss: 6841.49169705269\n",
      "Epoch 154\n",
      "Train Loss: 6841.485503663365\n",
      "Epoch 155\n",
      "Train Loss: 6841.48010086792\n",
      "Epoch 156\n",
      "Train Loss: 6841.473348800133\n",
      "Epoch 157\n",
      "Train Loss: 6841.4670794135345\n",
      "Epoch 158\n",
      "Train Loss: 6841.461147594197\n",
      "Epoch 159\n",
      "Train Loss: 6841.455605072357\n",
      "Epoch 160\n",
      "Train Loss: 6841.450496631199\n",
      "Epoch 161\n",
      "Train Loss: 6841.445442712578\n",
      "Epoch 162\n",
      "Train Loss: 6841.441585951512\n",
      "Epoch 163\n",
      "Train Loss: 6841.436099439225\n",
      "Epoch 164\n",
      "Train Loss: 6841.431675542621\n",
      "Epoch 165\n",
      "Train Loss: 6841.427633633501\n",
      "Epoch 166\n",
      "Train Loss: 6841.424350384251\n",
      "Epoch 167\n",
      "Train Loss: 6841.41951967931\n",
      "Epoch 168\n",
      "Train Loss: 6841.41577505337\n",
      "Epoch 169\n",
      "Train Loss: 6841.4122836116185\n",
      "Epoch 170\n",
      "Train Loss: 6841.409052751199\n",
      "Epoch 171\n",
      "Train Loss: 6841.406001416492\n",
      "Epoch 172\n",
      "Train Loss: 6841.402812718594\n",
      "Epoch 173\n",
      "Train Loss: 6841.399585514191\n",
      "Epoch 174\n",
      "Train Loss: 6841.396913791584\n",
      "Epoch 175\n",
      "Train Loss: 6841.394191790441\n",
      "Epoch 176\n",
      "Train Loss: 6841.392091104833\n",
      "Epoch 177\n",
      "Train Loss: 6841.389130128223\n",
      "Epoch 178\n",
      "Train Loss: 6841.387357643256\n",
      "Epoch 179\n",
      "Train Loss: 6841.383602312467\n",
      "Epoch 180\n",
      "Train Loss: 6841.381731010548\n",
      "Epoch 181\n",
      "Train Loss: 6841.379144413553\n",
      "Epoch 182\n",
      "Train Loss: 6841.3769921698\n",
      "Epoch 183\n",
      "Train Loss: 6841.374995410691\n",
      "Epoch 184\n",
      "Train Loss: 6841.373380100088\n",
      "Epoch 185\n",
      "Train Loss: 6841.371658436972\n",
      "Epoch 186\n",
      "Train Loss: 6841.369628014471\n",
      "Epoch 187\n",
      "Train Loss: 6841.367745536282\n",
      "Epoch 188\n",
      "Train Loss: 6841.36629207871\n",
      "Epoch 189\n",
      "Train Loss: 6841.364585245804\n",
      "Epoch 190\n",
      "Train Loss: 6841.36303114185\n",
      "Epoch 191\n",
      "Train Loss: 6841.361620863114\n",
      "Epoch 192\n",
      "Train Loss: 6841.360210294755\n",
      "Epoch 193\n",
      "Train Loss: 6841.359559936869\n"
     ]
    }
   ],
   "source": [
    "p: int = 1\n",
    "sgd_1_w: np.ndarray; sgd_1_history: pd.DataFrame\n",
    "sgd_1_w, sgd_1_history = train(train_K, train_Y, lamb, lr, opt, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 6925.151593853819\n",
      "Epoch 2\n",
      "Train Loss: 6914.61983207711\n",
      "Epoch 3\n",
      "Train Loss: 6905.215065798347\n",
      "Epoch 4\n",
      "Train Loss: 6903.504096967731\n",
      "Epoch 5\n",
      "Train Loss: 6900.396073281848\n",
      "Epoch 6\n",
      "Train Loss: 6895.613089416973\n",
      "Epoch 7\n",
      "Train Loss: 6893.620467388609\n",
      "Epoch 8\n",
      "Train Loss: 6893.591814930905\n",
      "Epoch 9\n",
      "Train Loss: 6893.587430211146\n",
      "Epoch 10\n",
      "Train Loss: 6893.587425045763\n",
      "Epoch 11\n",
      "Train Loss: 6893.587415452174\n",
      "Epoch 12\n",
      "Train Loss: 6893.587397904866\n",
      "Epoch 13\n",
      "Train Loss: 6893.587368023076\n",
      "Epoch 14\n",
      "Train Loss: 6893.587330061126\n",
      "Epoch 15\n",
      "Train Loss: 6893.587320617856\n",
      "Epoch 16\n",
      "Train Loss: 6893.587316412312\n"
     ]
    }
   ],
   "source": [
    "opt = \"bfgs\"\n",
    "bfgs_w: np.ndarray; bfgs_history: pd.DataFrame\n",
    "bfgs_w, bfgs_history = train(bfgs_K, train_Y, lamb, lr, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 6921.71396657739\n",
      "Epoch 2\n",
      "Train Loss: 6863.651624172415\n",
      "Epoch 3\n",
      "Train Loss: 6841.330000466706\n",
      "Epoch 4\n",
      "Train Loss: 6841.328791541861\n",
      "Epoch 5\n",
      "Train Loss: 6841.327828432469\n",
      "Epoch 6\n",
      "Train Loss: 6841.327559005026\n",
      "Epoch 7\n",
      "Train Loss: 6841.327537237943\n"
     ]
    }
   ],
   "source": [
    "opt = \"lbfgs\"\n",
    "lbfgs_w: np.ndarray; lbfgs_history: pd.DataFrame\n",
    "lbfgs_w, lbfgs_history = train(train_K, train_Y, lamb, lr, opt, maxcor=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gd_history.index, gd_history['Train Loss'], label=\"GD\")\n",
    "plt.plot(sgd_1_history.index, sgd_1_history['Train Loss'], label=\"SGD, p=1\")\n",
    "plt.plot(sgd_100_history.index, sgd_100_history['Train Loss'], label=\"SGD, p=100\")\n",
    "plt.plot(bfgs_history.index, bfgs_history['Train Loss'], label=\"BFGS\")\n",
    "plt.plot(lbfgs_history.index, lbfgs_history['Train Loss'], label=\"LBFGS\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Loss Values of Optimizers per Iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w: np.ndarray, k: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"Gets the accuracy of the weight vector w on kernel k with targets y.\"\n",
    "    p: np.ndarray = sigmoid(k.T.dot(w))\n",
    "    y_pred: np.ndarray = (p >= 0.5).astype(int) * 2 - 1\n",
    "    return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Train Accuracy: 78.91%\n",
      "GD Test Accuracy: 81.40%\n",
      "SGD Train Accuracy, p=1: 78.96%\n",
      "SGD Test Accuracy, p=1: 81.50%\n",
      "SGD Train Accuracy, p=100: 78.97%\n",
      "SGD Test Accuracy, p=100: 81.50%\n",
      "BFGS Train Accuracy: 54.50%\n",
      "BFGS Test Accuracy: 54.50%\n",
      "LBFGS Train Accuracy: 78.91%\n",
      "LBFGS Test Accuracy: 81.40%\n"
     ]
    }
   ],
   "source": [
    "print(f'GD Train Accuracy: {accuracy(gd_w, train_K, train_Y) * 100:.2f}%')\n",
    "print(f'GD Test Accuracy: {accuracy(gd_w, test_K, test_Y) * 100:.2f}%')\n",
    "print(f'SGD Train Accuracy, p=1: {accuracy(sgd_1_w, train_K, train_Y) * 100:.2f}%')\n",
    "print(f'SGD Test Accuracy, p=1: {accuracy(sgd_1_w, test_K, test_Y) * 100:.2f}%')\n",
    "print(f'SGD Train Accuracy, p=100: {accuracy(sgd_100_w, train_K, train_Y) * 100:.2f}%')\n",
    "print(f'SGD Test Accuracy, p=100: {accuracy(sgd_100_w, test_K, test_Y) * 100:.2f}%')\n",
    "print(f'BFGS Train Accuracy: {accuracy(bfgs_w, bfgs_K, train_Y) * 100:.2f}%')\n",
    "print(f'BFGS Test Accuracy: {accuracy(bfgs_w, bfgs_test_K, test_Y) * 100:.2f}%')\n",
    "print(f'LBFGS Train Accuracy: {accuracy(lbfgs_w, train_K, train_Y) * 100:.2f}%')\n",
    "print(f'LBFGS Test Accuracy: {accuracy(lbfgs_w, test_K, test_Y) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
